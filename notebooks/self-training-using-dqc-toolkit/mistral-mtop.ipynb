{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by installing and loading the required dependencies. We will require the Python version to be ≥ 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install huggingface_hub\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "import warnings\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "wandb.init(mode=\"disabled\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using MTOP Domain (English), a publicly available dataset hosted on Hugging Face. It consists of user utterances from one of eleven possible domains  shown below - \n",
    "\n",
    "['messaging', 'calling', 'event', 'timer', 'music', 'weather', 'alarm', 'people', 'reminder', 'recipes', 'news']\n",
    "\n",
    "The dataset has 15,667 training samples and 2,235 validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "\n",
    "dataset = 'mteb/mtop_domain'\n",
    "dset = load_dataset(dataset, trust_remote_code=True)\n",
    "train_data = pd.DataFrame(dset['train'])\n",
    "val_data = pd.DataFrame(dset['validation'])\n",
    "print(f\"Num train samples : {len(train_data)}\")\n",
    "print(f\"Num validation samples : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of benchmarking our experiments, we choose Weighted F1 score as the metric. We also display the classification report and confusion matrix for detailed interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                              ConfusionMatrixDisplay, f1_score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fetch_performance_metrics(y_true: np.ndarray, y_pred: np.ndarray, exp_name: str,\n",
    "                              display_report: bool = True, display_confusion_matrix: bool = True,\n",
    "                             label_list: List[str] = ['messaging', 'calling', 'event', 'timer', 'music', 'weather',\n",
    "                                                      'alarm', 'people', 'reminder', 'recipes', 'news'],\n",
    "                              num_labels: int = 11) -> dict:\n",
    "    \"\"\"\n",
    "    Util function to compute F1 score and optionally display the classification report and confusion matrix for a given experiment.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Array containing true labels.\n",
    "        y_pred (np.ndarray): Array containing predicted labels.\n",
    "        exp_name (str): Name of the experiment (used to save results).\n",
    "        display_report (bool, optional): Boolean flag indicating whether to display classification report (True) or not (False). Defaults to True.\n",
    "        display_confusion_matrix (bool, optional): Boolean flag indicating whether to display confusion matrix  (True) or not (False). Defaults to True.\n",
    "        label_list (list, optional): List of labels. Defaults to ['messaging', 'calling', 'event', 'timer', 'music', 'weather',\n",
    "                                                      'alarm', 'people', 'reminder', 'recipes', 'news'].\n",
    "        num_labels (int, optional): Number of unique labels. Defaults to 11.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing F1 score.\n",
    "    \"\"\" \n",
    "    if display_report:\n",
    "        print('\\nClassification Report:')\n",
    "        \n",
    "        print(classification_report(y_true=y_true, y_pred=y_pred, labels=list(range(num_labels)),\n",
    "                                   target_names=label_list[:num_labels]))\n",
    "    \n",
    "    if display_confusion_matrix:\n",
    "        cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_list)\n",
    "        display.plot(ax=ax)\n",
    "        plt.savefig(exp_name)\n",
    "        \n",
    "    return {'F1-score' : f1_score(y_true, y_pred, average='weighted')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline : LLM with ICL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to login to Hugging Face hub to be able to access the LLM. We do this via Hugging Face's notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LLM Pre-liminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                            BitsAndBytesConfig, pipeline)\n",
    "\n",
    "import datasets\n",
    "import gc\n",
    "\n",
    "def _generate_predictions(example: datasets.formatting.formatting.LazyBatch, \n",
    "                          generator: pipeline, text_column: str, \n",
    "                          max_new_tokens: int = 9) -> dict:\n",
    "    \"\"\"\n",
    "    Generates predictions using the text generation model for a given example.\n",
    "\n",
    "    Args:\n",
    "        example (datasets.formatting.formatting.LazyBatch): Batch of samples from a dataset.\n",
    "        generator (pipeline): Huggingface pipeline for text generation.\n",
    "        text_column (str): Prompt for the text generation model.\n",
    "        max_new_tokens (int, optional): Maximum number of tokens to generate. Defaults to 9.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated predictions.\n",
    "    \"\"\"\n",
    "    num_examples = len(dataset)\n",
    "    predictions = []\n",
    "    batch_results = generator(example[text_column], max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    predictions.extend([result[0][\"generated_text\"] for result in batch_results])\n",
    "    return {'prediction' : predictions}\n",
    "\n",
    "def infer_LLM(model_name: str, input_ds: Dataset, batch_size: int = 4, max_new_tokens: int = 9,\n",
    "             text_column: str = 'domain_prompt', finetuned_model_path: str = None) -> Dataset:\n",
    "    \"\"\"\n",
    "    Util function to run LLM inference\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name or path of the LLM model.\n",
    "        input_ds (Dataset): Input dataset containing text prompts.\n",
    "        batch_size (int, optional): Batch size for inference. Defaults to 4.\n",
    "        max_new_tokens (int, optional): Maximum number of tokens to generate. Defaults to 9.\n",
    "        text_column (str, optional): Name of the column containing text prompts. Defaults to 'domain_prompt'.\n",
    "        finetuned_model_path (str, optional): Path to the fine-tuned model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dataset: Dataset with generated predictions.\n",
    "    \"\"\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    \n",
    "    if finetuned_model_path is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",\n",
    "                                                quantization_config=quantization_config)\n",
    "    else:\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(finetuned_model_path,\n",
    "                                                        device_map=\"auto\",\n",
    "                                                quantization_config=quantization_config)\n",
    "    \n",
    "    text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\n",
    "                             batch_size=batch_size, truncation=False)\n",
    "    text_generator.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    input_ds = input_ds.map(_generate_predictions, fn_kwargs={'generator' : text_generator,\n",
    "                                                              'text_column' : text_column,\n",
    "                                                              'max_new_tokens' : max_new_tokens\n",
    "                                                             },\n",
    "                           batched=True, batch_size=batch_size)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    del text_generator\n",
    "    del tokenizer\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return input_ds\n",
    "\n",
    "def build_LLM_prompt(input_ds: Dataset, label_column: str = None, prompt_template: Union[str, None] = None, \n",
    "                     with_label: bool = False) -> Dataset:\n",
    "    \"\"\"Util function to build the LLM prompt from input text data\n",
    "\n",
    "    Args:\n",
    "        input_ds (Dataset): Input dataset containing text\n",
    "        label_column (str, optional): Label column in the data. Applicable if constructing prompts for in-context samples / finetuning LLM. Defaults to None.\n",
    "        prompt_template (Union[str, None], optional): Text instruction to prepend to each transformed input text sample. Defaults to None.\n",
    "        with_label (bool, optional): `True` if the prompts should include labels from the `label_column`. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Dataset with generated predictions.\n",
    "    \"\"\"\n",
    "    if type(input_ds) == pd.DataFrame:\n",
    "        input_ds = Dataset.from_pandas(input_ds)\n",
    "        \n",
    "    if with_label:\n",
    "        \n",
    "        input_ds = input_ds.map(lambda x: {'domain_prompt': '[UTTERANCE]' + x['text'] + '[/UTTERANCE]' + \\\n",
    "                                          '[DOMAIN]' + x[label_column] + '[/DOMAIN]'})\n",
    "    else:\n",
    "        input_ds = input_ds.map(lambda x: {'domain_prompt': prompt_template + '[UTTERANCE]' + x['text'] + '[/UTTERANCE]' + \\\n",
    "                                          '[DOMAIN]'})\n",
    "    \n",
    "    return input_ds\n",
    "\n",
    "def _extract_label(sample: datasets.formatting.formatting.LazyRow, label_list: List[str]) -> dict:\n",
    "    \"\"\"Util function to extract the domain from the generated LLM prediction\n",
    "\n",
    "    Args:\n",
    "        sample (datasets.formatting.formatting.LazyRow): Batch of samples from a dataset\n",
    "        label_list (List[str]): List of possible domains\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of extracted predicted labels\n",
    "    \"\"\"\n",
    "    prompt_length = len(sample['domain_prompt'])\n",
    "    generated_answer = sample['prediction'][prompt_length:].split('[/DOMAIN]')[0].lower()\n",
    "\n",
    "    label_matched = False\n",
    "    predicted_label = None\n",
    "    \n",
    "    for label in label_list:        \n",
    "        if label in generated_answer:\n",
    "            predicted_label = label\n",
    "            label_matched = True\n",
    "            break\n",
    "                \n",
    "    if not label_matched:\n",
    "        predicted_label = \"no_match\"\n",
    "    \n",
    "    return {'predicted_label' : predicted_label}\n",
    "     \n",
    "def run_llm(val_data: pd.DataFrame, prompt_template: str, model_name: str, domain_list: List[str], label_mapping: dict, \n",
    "            label_column: str = 'label', batch_size: int = 4, finetuned_model_path: str = None,\n",
    "           num_labels: int = 11, compute_metrics: bool = True) -> dict:\n",
    "    \"\"\"Run end-to-end LLM inference (from pre-processing input data to post-processing the predictions) and return the computed performance metrics on input validation data\n",
    "\n",
    "    Args:\n",
    "        val_data (pd.DataFrame): Validation data with labels\n",
    "        prompt_template (str): Text instruction to prepend to each transformed input text sample.\n",
    "        model_name (str): The name or path of the pre-trained LLM.\n",
    "        domain_list (List[str]): List of possible domains \n",
    "        label_mapping (dict): Dictionary mapping to convert text labels to integers \n",
    "        label_column (str, optional): Label column in the data. Defaults to 'label'.\n",
    "        batch_size (int, optional): Batch size for inference. Defaults to 4.\n",
    "        finetuned_model_path (str, optional):  Path to the fine-tuned model, if available.. Defaults to None.\n",
    "        num_labels (int, optional): Number of unique labels. Defaults to 6.\n",
    "        compute_metrics (bool, optional): Boolean flag indicating whether to compute the performance metrics (True) or not (False)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing F1 score.\n",
    "    \"\"\"\n",
    "    predicted_label_list = []\n",
    "    val_ds = build_LLM_prompt(val_data, prompt_template=prompt_template)\n",
    "    val_ds_with_pred = infer_LLM(model_name, val_ds, batch_size, finetuned_model_path=finetuned_model_path)\n",
    "    \n",
    "    predicted_label_list = val_ds_with_pred.map(_extract_label, \n",
    "                                  fn_kwargs={\"label_list\": domain_list[:num_labels]})['predicted_label'] \n",
    "\n",
    "    y_pred = [label_mapping[pred] if pred in label_mapping else num_labels for pred in predicted_label_list]\n",
    "    y_true = val_data[label_column].astype(int).values.tolist()\n",
    "\n",
    "    if num_labels not in y_pred:\n",
    "        # All LLM predictions match a valid domain from `domain_list`\n",
    "        domain_list.remove('no_match')\n",
    "    \n",
    "    if compute_metrics:\n",
    "        return y_pred, fetch_performance_metrics(y_true, y_pred, 'mistral_7b', label_list=domain_list)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `build_LLM_prompt` transforms the input text into a LLM prompt\n",
    "* `infer_LLM` and `_generate_predictions` instantiate the LLM and run inference with the constructed input prompts.\n",
    "* `_extract_label` maps the LLM free text outputs to valid domain predictions. If the generated text has no matching domain, the predicted label is set to \"no_match\".\n",
    "* `run_LLM` invokes functions `build_LLM_prompt` and `infer_LLM` to perform inference and return the computed performance metrics on input validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LLM prompt \n",
    "We select one sample at random for each label and build the prompt prefix to run ICL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "seed = 43\n",
    "\n",
    "sample_data = train_data.groupby('label_text').sample(n=1, random_state=seed).reset_index(drop=True)\n",
    "domain_list = ['messaging', 'calling', 'event', 'timer', 'music', 'weather',\n",
    "                'alarm', 'people', 'reminder', 'recipes', 'news']\n",
    "domain_list_str = ', '.join(domain_list)\n",
    "\n",
    "transformed_sample_data = build_LLM_prompt(sample_data, with_label=True, label_column='label_text')\n",
    "samples_str = '\\n'.join(transformed_sample_data['domain_prompt'])\n",
    "\n",
    "prompt_template =  \"<s>[INST] You are a helpful, respectful and honest assistant. Choose one option that best describes the domain behind the given utterance based on the following comma separated options: \" + domain_list_str + \"[/INST] </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all to work\n",
    "We are ready to run our LLM now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = train_data[['label', 'label_text']].drop_duplicates().sort_values(by='label')\n",
    "text_to_label = dict(zip(mapping_df['label_text'], mapping_df['label']))\n",
    "llm_domain_list = domain_list + ['no_match']\n",
    "\n",
    "_, score = run_llm(val_data, prompt_template, model_name, llm_domain_list, text_to_label,\n",
    "        batch_size=64)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Approach : Self-Training using DQC Toolkit\n",
    "Our proposed self-training approach is comprised of the following three steps - \n",
    "1. Generate LLM Predictions on Unlabelled Data\n",
    "2. Apply Label Correction using DQC Toolkit\n",
    "3. Fine-tune LLM using Reliably Labelled Data \n",
    "### Step 1 - Generate LLM Predictions on Unlabelled Data\n",
    "We leverage LLM with ICL to generate initial labels for the data to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = run_llm(train_data, prompt_template, model_name, llm_domain_list, text_to_label,\n",
    "                      batch_size=64, compute_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, many predictions can end up being mapped to \"no_match\". We remove such samples from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['llm_predicted_label'] = pd.Series(predictions)\n",
    "## Only valid label predictions\n",
    "train_data_with_llm_pred = train_data.loc[train_data['llm_predicted_label'] < len(domain_list), ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Apply Label Correction using DQC Toolkit\n",
    "Currently, DQC toolkit offers `CrossValCurate` for curation of text classification datasets (binary / multi-class) using cross validation based label prediction. We will leverage this module to acquire better quality labels for our data.\n",
    "\n",
    "We install DQC Toolkit via pypi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dqc-toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we curate the data and return corrected predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqc import CrossValCurate\n",
    "cvc = CrossValCurate(random_state=seed, \n",
    "                     calibration_method='calibrate_using_baseline' )\n",
    "\n",
    "train_data_curated = cvc.fit_transform(train_data_with_llm_pred, y_col_name='llm_predicted_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CrossValCurate` accepts two parameters *`random_state`* (random seed for reproducibility) and *`calibration_method`*(whether/how to calibrate the prediction probabilities of the model being trained for label correction). You can check out all the hyper-parameters available for the modules in the documentation [here](https://sumanthprabhu.github.io/DQC-Toolkit/latest/api/crossval/).\n",
    "\n",
    "The returned object ``train_data_curated`` is a Pandas dataframe similar to the input dataframe ``train_data_with_llm_pred`` with the following additional columns -\n",
    "* '*`label_correctness_score`*' represents a normalized score quantifying the correctness of *`llm_predicted_label`*.\n",
    "* '*`is_label_correct`*' is a boolean flag indicating whether the *`llm_predicted_label`* is to be considered correct (True) or incorrect (False).\n",
    "* '*`predicted_label`*' and '*`prediction_probability`*' represent DQC Toolkit's predicted label for a given sample and the corresponding probability score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leverage *`is_label_correct`* to identify reliably labelled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_curated = train_data_curated.loc[train_data_curated['is_label_correct']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Fine-tune LLM using Reliably Labelled Data\n",
    "We fine-tune the LLM Using ``train_data_curated`` with *`llm_predicted_label`* as the target variable. First, we map the integer labels to text labels for LLM interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_text = {v: k for k, v in text_to_label.items()}\n",
    "train_data_curated['llm_predicted_label_text'] = train_data_curated['llm_predicted_label'].map(label_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the data into instruction prompts for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template =  \"<s>[INST] You are a helpful, respectful and honest assistant. Choose one option that best describes the domain that can be mapped to the given utterance based on the following comma separated options: \" + domain_list_str + \"[/INST] </s>\"\n",
    "label_column = 'llm_predicted_label_text'\n",
    "\n",
    "train_data_curated_ds = build_LLM_prompt(train_data_curated, with_label=True, label_column=label_column)\n",
    "train_data_curated_ds = train_data_curated_ds.map(lambda example, prompt_template=prompt_template : {'domain_prompt' : prompt_template + example['domain_prompt']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the LLM fine-tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, \n",
    "                          BitsAndBytesConfig, DataCollatorForLanguageModeling,\n",
    "                          pipeline, Trainer, TrainingArguments \n",
    "                          )\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "\n",
    "def tokenize(example: datasets.formatting.formatting.LazyRow, tokenizer: AutoTokenizer ) -> dict:\n",
    "    \"\"\"Util function to tokenize text data\n",
    "\n",
    "    Args:\n",
    "        example (datasets.formatting.formatting.LazyRow): Batch of samples containing text to tokenize.\n",
    "        tokenizer (AutoTokenizer): Tokenizer object used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing tokenized text.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        example['domain_prompt'],\n",
    "        truncation=False\n",
    "    )\n",
    "    \n",
    "    return {**tokenized}\n",
    "    \n",
    "def finetune_LLM(base_model_name: str, train_ds: Dataset,\n",
    "              save_path: str, seed: int, batch_size: int = 64, num_epochs: int = 1):\n",
    "    \"\"\"Function to fine-tune an LLM on the given input training data\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): The name or path of the LLM model to be finetuned\n",
    "        train_ds (Dataset): Input dataset containing text prompts.\n",
    "        save_path (str): Path to save the trained model\n",
    "        seed (int): Random seed for reproducibility\n",
    "        batch_size (int, optional): Batch size to use during training. Defaults to 64.\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 1.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name, \n",
    "                                                 quantization_config=bnb_config, \n",
    "                                                 device_map=\"auto\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    train_ds = train_ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "            disable_tqdm=False,\n",
    "            output_dir=save_path,\n",
    "            warmup_steps=1,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            optim=\"paged_adamw_8bit\",             \n",
    "            logging_dir=\"./logs\",        \n",
    "            save_strategy=\"no\",              \n",
    "            evaluation_strategy=\"no\",                             \n",
    "            report_to=None          \n",
    "        )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_ds.select_columns(['input_ids', 'attention_mask']),\n",
    "        eval_dataset=None,\n",
    "        args=args,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(save_path)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del trainer\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to fine-tune the model. The number of training epochs is set to 1 and batch size is set to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "finetuned_model_path = \"selftrained-mistral-mtop\"\n",
    "finetune_LLM(model_name, train_data_curated_ds, save_path=finetuned_model_path, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Self-Trained Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, score = run_llm(val_data, prompt_template, model_name, llm_domain_list, text_to_label,\n",
    "       finetuned_model_path=finetuned_model_path, batch_size=64)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
