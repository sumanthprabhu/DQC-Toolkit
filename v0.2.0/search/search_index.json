{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DQC toolkit","text":"<p>DQC Toolkit is a Python library and framework designed with the goal to facilitate improvement of Machine Learning models by identifying and mitigating label errors in training dataset. Currently, DQC toolkit offers <code>CrossValCurate</code> and <code>LLMCurate</code>. <code>CrossValCurate</code> can be used for label error detection / correction in text classification (binary / multi-class) based on cross validation. <code>LLMCurate</code> extends PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars to compute LLM-based confidence scores for free-text labels.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation of DQC-toolkit can be done as shown below <pre><code>pip install dqc-toolkit\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#crossvalcurate","title":"CrossValCurate","text":"<p>Assuming your text classification data is stored as a pandas dataframe <code>data</code>, with each sample represented by the <code>text</code> column and its corresponding noisy label represented by the <code>label</code> column,  here is how you use <code>CrossValCurate</code> - </p> <p><pre><code>from dqc import CrossValCurate\n\ncvc = CrossValCurate()\ndata_curated = cvc.fit_transform(data[['text', 'label']])\n</code></pre> The result stored in <code>data_curated</code> is a pandas dataframe similar to <code>data</code> with the following columns - <pre><code>&gt;&gt;&gt; data_curated.columns\n['text', 'label', 'label_correctness_score', 'is_label_correct', 'predicted_label', 'prediction_probability']\n</code></pre></p> <ul> <li><code>'label_correctness_score'</code> represents a normalized score quantifying the correctness of <code>'label'</code>. </li> <li><code>'is_label_correct'</code> is a boolean flag indicating whether the given <code>'label'</code> is correct (<code>True</code>) or incorrect (<code>False</code>). </li> <li><code>'predicted_label'</code> and <code>'prediction_probability'</code> represent the curation model's prediction and the corresponding probability score. </li> </ul>"},{"location":"#llmcurate","title":"LLMCurate","text":"<p>Assuming <code>data</code> is a pandas dataframe containing samples with our target text for curation under column <code>column_to_curate</code>, here is how you use <code>LLMCurate</code> - <pre><code>llmc = LLMCurate(model, tokenizer)\nds = llmc.run(\n        data,\n        column_to_curate,\n        ds_column_mapping,\n        prompt_variants,\n        llm_response_cleaned_column_list,\n        answer_start_token,\n        answer_end_token,\n        batch_size,\n        max_new_tokens\n        )\n</code></pre> where</p> <ul> <li><code>model</code> and <code>tokenizer</code> are the instantiated LLM model and tokenizer objects respectively</li> <li><code>ds_column_mapping</code> is the dictionary mapping of entities used in the LLM prompt and the corresponding columns in <code>data</code>. For example, <code>ds_column_mapping={'INPUT' : 'input_column'}</code> would imply that text under <code>input_column</code> in <code>data</code> would be passed to the LLM in the format <code>\"[INPUT]row['input_column'][/INPUT]\"</code> for each <code>row</code> in <code>data</code> </li> <li><code>prompt_variants</code> is the list of LLM prompts to be used to curate <code>column_to_curate</code> and <code>llm_response_cleaned_column_list</code> is the corresponding list of column names to store the reference responses generated using each prompt</li> <li><code>answer_start_token</code> and <code>answer_end_token</code> are optional  text phrases representing the start and end of the answer respectively.</li> </ul> <p><code>ds</code> is a dataset object with the following additional features -</p> <ol> <li>Feature for each column name in <code>llm_response_cleaned_column_list</code></li> <li>LLM Confidence score for each text in <code>column_to_curate</code> </li> </ol> <p>For more details regarding different hyperparameters available in <code>CrossValCurate</code> and <code>LLMCurate</code>, please refer to the API documentation.</p>"},{"location":"api/crossval/","title":"Cross Validation based Curation","text":""},{"location":"api/crossval/#basecurate","title":"BaseCurate","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for data curation to compute label correctness scores   and identify reliable labelled samples</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Random seed for                     reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>dqc/base.py</code> <pre><code>class BaseCurate(ABC):\n    \"\"\"Base class for data curation to compute label correctness scores\n      and identify reliable labelled samples\n\n    Args:\n        random_state (RandomState, optional): Random seed for\n                                reproducibility. Defaults to 42.\n    \"\"\"\n\n    def __init__(\n        self,\n        random_state: Union[int, RandomState] = 42,\n        **options,\n    ):\n        self.random_state = random_state\n        self._set_seed(random_state)\n\n    def _set_seed(self, seed):\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n\n    @abstractmethod\n    def fit_transform(self): ...\n</code></pre>"},{"location":"api/crossval/#crossvalcurate","title":"CrossValCurate","text":"<p>             Bases: <code>BaseCurate</code></p> <p>Parameters:</p> Name Type Description Default <code>curate_feature_extractor</code> <code>Union[str, TfidfVectorizer, CountVectorizer, SentenceTransformer]</code> <p>Feature extraction method to be used during curation. Accepts string values 'TfidfVectorizer', 'CountVectorizer' which map to <code>sklearn.feature_extraction.text.TfidfVectorizer</code> and <code>sklearn.feature_extraction.text.CountVectorizer</code> objects respectively. Also accepts explicit instances of <code>sklearn.feature_extraction.text.TfidfVectorizer</code> / <code>sklearn.feature_extraction.text.CountVectorizer</code>.  Additionally, supports <code>SentenceTransformer</code> model object or a string matching the name of a <code>SentenceTransformer</code> model hosted on HuggingFace Model Hub. Defaults to 'TfidfVectorizer'.</p> <code>'TfidfVectorizer'</code> <code>curate_model</code> <code>str</code> <p>Machine learning model that is trained with <code>curate_feature_extractor</code> based features during curation. Accepts any instance of scikit-learn classifier that implement <code>predict_proba()</code> method. Defaults to <code>sklearn.linear_model.LogisticRegression</code>.</p> <code>LogisticRegression()</code> <code>calibration_method</code> <code>Union[str, None]</code> <p>Approach to be used for calibration of <code>curate_model</code> predictions. Defaults to 'calibrate_using_baseline'.</p> <code>'calibrate_using_baseline'</code> <code>correctness_threshold</code> <code>float</code> <p>Minimum prediction probability using <code>curate_model</code> to consider the corresponding sample as 'correctly labelled'. Defaults to 0.0.</p> <code>0.0</code> <code>n_splits</code> <code>int</code> <p>Number of splits to use when running cross-validation based curation.</p> <code>5</code> <code>verbose</code> <code>bool</code> <p>Sets the verbosity level during execution. <code>True</code> indicates logging level INFO and <code>False</code> indicates logging level 'ERROR'.</p> <code>False</code> <p>Examples: Assuming <code>data</code> is a pandas dataframe containing samples with noisy labels, here is how you would use <code>CrossValCurate</code> - <pre><code>from dqc import CrossValCurate\n\ncvc = CrossValCurate()\ndata_curated = cvc.fit_transform(data[['text', 'label']])\n</code></pre></p> <p><code>data_curated</code> is a pandas dataframe similar to <code>data</code> with the following columns - <pre><code>&gt;&gt;&gt; data_curated.columns\n['text', 'label', 'label_correctness_score', 'is_label_correct', 'predicted_label', 'prediction_probability']\n</code></pre></p> <p><code>'label_correctness_score'</code> represents a normalized score quantifying the correctness of <code>'label'</code>. </p> <p><code>'is_label_correct'</code> is a boolean flag indicating whether the given <code>'label'</code> is correct (<code>True</code>) or incorrect (<code>False</code>). </p> <p><code>'predicted_label'</code> and <code>'prediction_probability'</code> represent the curation model's prediction and the corresponding probability score.</p> Source code in <code>dqc/crossval.py</code> <pre><code>class CrossValCurate(BaseCurate):\n    \"\"\"\n    Args:\n        curate_feature_extractor (Union[str, TfidfVectorizer, CountVectorizer, SentenceTransformer], optional): Feature extraction method to be used during curation. Accepts string values 'TfidfVectorizer', 'CountVectorizer' which map to `sklearn.feature_extraction.text.TfidfVectorizer` and `sklearn.feature_extraction.text.CountVectorizer` objects respectively. Also accepts explicit instances of `sklearn.feature_extraction.text.TfidfVectorizer` / `sklearn.feature_extraction.text.CountVectorizer`.  Additionally, supports `SentenceTransformer` model object or a string matching the name of a `SentenceTransformer` model hosted on HuggingFace Model Hub. Defaults to 'TfidfVectorizer'.\n        curate_model (str, optional): Machine learning model that is trained with `curate_feature_extractor` based features during curation. Accepts any instance of scikit-learn classifier that implement `predict_proba()` method. Defaults to `sklearn.linear_model.LogisticRegression`.\n        calibration_method (Union[str, None], optional): Approach to be used for calibration of `curate_model` predictions. Defaults to 'calibrate_using_baseline'.\n        correctness_threshold (float, optional): Minimum prediction probability using `curate_model` to consider the corresponding sample as 'correctly labelled'. Defaults to 0.0.\n        n_splits (int, optional): Number of splits to use when running cross-validation based curation.\n        verbose (bool, optional): Sets the verbosity level during execution. `True` indicates logging level INFO and `False` indicates logging level 'ERROR'.\n\n    Examples:\n    Assuming `data` is a pandas dataframe containing samples with noisy labels, here is how you would use `CrossValCurate` -\n    ```python\n\n    from dqc import CrossValCurate\n\n    cvc = CrossValCurate()\n    data_curated = cvc.fit_transform(data[['text', 'label']])\n    ```\n\n    `data_curated` is a pandas dataframe similar to `data` with the following columns -\n    ```python\n    &gt;&gt;&gt; data_curated.columns\n    ['text', 'label', 'label_correctness_score', 'is_label_correct', 'predicted_label', 'prediction_probability']\n    ```\n\n    `'label_correctness_score'` represents a normalized score quantifying the correctness of `'label'`. \\n\n    `'is_label_correct'` is a boolean flag indicating whether the given `'label'` is correct (`True`) or incorrect (`False`). \\n\n    `'predicted_label'` and `'prediction_probability'` represent the curation model's prediction and the corresponding probability score.\n    \"\"\"\n\n    def __init__(\n        self,\n        curate_feature_extractor: Union[\n            str, TfidfVectorizer, CountVectorizer, SentenceTransformerVectorizer\n        ] = \"TfidfVectorizer\",\n        curate_model: Union[str, ClassifierMixin] = LogisticRegression(),\n        calibration_method: Union[str, None] = \"calibrate_using_baseline\",\n        correctness_threshold: float = 0.0,\n        n_splits: int = 5,\n        verbose: bool = False,\n        **options,\n    ):\n        super().__init__(**options)\n\n        self.curate_feature_extractor, self.curate_model, self.calibration_method = (\n            _fetch_curation_artifacts(\n                curate_feature_extractor, curate_model, calibration_method\n            )\n        )\n\n        self.correctness_threshold = correctness_threshold\n        self.n_splits = n_splits\n        self.verbose = verbose\n        self._set_verbosity(verbose)\n\n        self.curate_pipeline = None\n        self.scaler = None\n        self.y_col_name_int = None\n        self.result_col_list = [\n            \"label_correctness_score\",\n            \"is_label_correct\",\n            \"predicted_label\",\n            \"prediction_probability\",\n        ]\n\n    def __str__(self):\n        display_dict = self.__dict__.copy()\n        for key in list(display_dict.keys()):\n            if key in [\n                \"curate_pipeline\",\n                \"scaler\",\n                \"y_col_name_int\",\n                \"result_col_list\",\n            ]:\n                ## Don't need to display these attributes\n                del display_dict[key]\n\n        return str(display_dict)\n\n    __repr__ = __str__\n\n    def _is_confident(self, row: pd.Series) -&gt; bool:\n        \"\"\"Return a boolean variable indicating whether we are confident\n           about the correctness of the label assigned to a given data sample\n\n        Args:\n            row (pd.Series): data sample whose label correctness need to be evaluated\n\n        Returns:\n            bool: `True` if we are confident that the label assigned is correct\n                else `False`\n        \"\"\"\n        threshold = self.correctness_threshold\n        if (row[\"predicted_label_int\"] == row[self.y_col_name_int]) and (\n            row[\"label_correctness_score\"] &gt;= threshold\n        ):\n            return True\n        return False\n\n    def _set_verbosity(self, verbose: bool):\n        \"\"\"Set logger level based on user input for parameter `verbose`\n\n        Args:\n            verbose (bool): Indicator for verbosity\n        \"\"\"\n        if verbose:\n            logger.set_level(\"INFO\")\n        else:\n            logger.set_level(\"WARNING\")\n\n    def _no_calibration(\n        self,\n        input_labels: List[Union[int, str]],\n        pred_prob_matrix: np.ndarray,\n        label_list: List[Union[int, str]],\n    ) -&gt; Tuple[List[Union[int, str]], List[float]]:\n        \"\"\"Returns predictions, corresponding probabilities and label correctness\n        scores without any calibration\n\n        Args:\n            input_labels (List[Union[int, str]]): _description_\n            pred_prob_matrix (np.ndarray): _description_\n            label_list (List[Union[int, str]]): _description_\n\n        Returns:\n            Tuple[List[Union[int, str]], List[float]]: _description_\n        \"\"\"\n        pred_probs = np.max(pred_prob_matrix, axis=1).tolist()\n        preds = [label_list[index] for index in np.argmax(pred_prob_matrix, axis=1)]\n        label_correctness_scores = [\n            pred_prob_matrix[row_index, label_list.index(label)]\n            for row_index, label in enumerate(input_labels)\n        ]\n\n        return preds, pred_probs, label_correctness_scores\n\n    def _get_baselines(self, input_data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Computes the baseline prediction probabilities using\n          input label distribution\n\n        Args:\n            input_data (pd.DataFrame): Input data with\n                                            corresponding noisy labels\n\n        Returns:\n            pd.DataFrame: Labels and corresponding probabilities\n        \"\"\"\n        thresholds_df = (input_data[self.y_col_name_int].value_counts(1)).reset_index()\n        thresholds_df.columns = [self.y_col_name_int, \"probability\"]\n        return thresholds_df\n\n    def _calibrate_using_baseline(\n        self,\n        input_labels: List[Union[int, str]],\n        pred_prob_matrix: np.ndarray,\n        label_list: List[Union[int, str]],\n        baseline_probs: pd.DataFrame,\n    ) -&gt; Tuple[List[Union[int, str]], List[float], List[float]]:\n        \"\"\"Calibrate the predicted probabilities using baseline probabilities and\n        returns the modified predictions, corresponding probabilities and label\n        correctness scores\n\n        Args:\n            input_labels (List[Union[int, str]]): Noisy labels from data\n            pred_prob_matrix (np.ndarray): Predicted probabilities per label\n                                for each sample in the data\n            label_list (List[Union[int, str]]): Ordered list of labels where\n                                ordering matches `pred_prob_matrix`\n            baseline_probs (pd.DataFrame): Prediction probabilities computed\n                                using input label distribution\n\n        Returns:\n            Tuple[List[Union[int, str]], List[float], List[float]]:\n            Returns the following\n            'calibrated_prediction' : Label predictions post calibration\n            'calibrated_probabilities' : Normalized scores corresponding\n                                    to 'calibrated_prediction'\n            'label_correctness_score' : Normalized scores corresponding\n                                     to 'label' (provided as input to\n                                    `self.fit_transform`)\n        \"\"\"\n\n        label_list_df = pd.DataFrame({\"label_list\": label_list})\n        baseline_probs = pd.merge(\n            label_list_df,\n            baseline_probs,\n            left_on=\"label_list\",\n            right_on=self.y_col_name_int,\n            how=\"inner\",\n        ).reset_index(drop=True)\n\n        baseline_probs.drop(\"label_list\", axis=1, inplace=True)\n\n        prob_array = baseline_probs[\"probability\"].values\n\n        # Calibrate prediction probabilities using baseline probabilities\n        pred_prob_matrix = (pred_prob_matrix - prob_array) / prob_array\n\n        pred_prob_matrix = normalize(pred_prob_matrix, norm=\"l2\")\n\n        calibrated_predictions = [\n            label_list[index] for index in np.argmax(pred_prob_matrix, axis=1)\n        ]\n        calibrated_probabilities = np.max(pred_prob_matrix, axis=1).tolist()\n        label_correctness_scores = [\n            pred_prob_matrix[row_index, label_list.index(label)]\n            for row_index, label in enumerate(input_labels)\n        ]\n\n        return (\n            calibrated_predictions,\n            calibrated_probabilities,\n            label_correctness_scores,\n        )\n\n    @_exception_handler\n    def fit_transform(\n        self,\n        data_with_noisy_labels: pd.DataFrame,\n        X_col_name: str = \"text\",\n        y_col_name: str = \"label\",\n        **options,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fit CrossValCurate on the given input data\n\n        Args:\n            data_with_noisy_labels (pd.DataFrame): Input data with corresponding noisy labels\n            X_col_name (str): Column to be used to extract input features.\n            y_col_name (str): Label column in the data.\n        Returns:\n\n            pd.DataFrame: Input Data samples with \\n\n                1) 'predicted_label' - Predicted labels using CrossValCurate \\n\n                2) 'prediction_probability' - Corresponding prediction probabilities \\n\n                3) 'label_correctness_score' - Label correctness scores based on prediction probabilities \\n\n                4) 'is_label_correct' - An indicator variable (True / False) classifying samples as correctly / incorrectly labelled.\n        \"\"\"\n        _is_valid(data_with_noisy_labels, X_col_name, y_col_name)\n        n_splits = self.n_splits\n        options[\"num_samples\"] = len(data_with_noisy_labels)\n\n        # Make a copy of `data_with_noisy_labels` to prevent accidental modification\n        input_data = data_with_noisy_labels.copy()\n        logger.info(\"Pre-processing the data..\")\n        dp = _DataProcessor(\n            random_state=self.random_state,\n        )\n\n        input_data, row_id_col, y_col_name_int, inv_label_mapping = dp._preprocess(\n            input_data, y_col_name=y_col_name\n        )\n\n        # y_col_name_int needs to be accessed downstream\n        self.y_col_name_int = y_col_name_int\n\n        data_columns = [X_col_name, y_col_name_int]\n        logger.info(\n            f\"Building the curation pipeline with {n_splits}-fold cross validation..\"\n        )\n        self.curate_pipeline = _get_pipeline(\n            self.curate_feature_extractor, self.curate_model, **options\n        )\n\n        split_indices = _data_splitter(\n            input_data,\n            X_col_name=X_col_name,\n            y_col_name_int=y_col_name_int,\n            n_splits=self.n_splits,\n        )\n\n        # Lists to store predictions\n        predictions = []\n        prediction_probabilities = []\n        label_correctness_scores = []\n\n        # Keep track of shuffled data\n        row_ids = []\n\n        if self.calibration_method == \"calibrate_using_baseline\":\n            logger.info(\"Computing baseline predictions for each label..\")\n            baseline_probs = self._get_baselines(input_data[data_columns])\n\n        # Iterate through kfold splits\n        for train_index, val_index in tqdm(split_indices):\n            # Split the data\n            X_train, X_val = (\n                input_data.loc[train_index, X_col_name].values,\n                input_data.loc[val_index, X_col_name].values,\n            )\n            y_train, y_val = (\n                input_data.loc[train_index, y_col_name_int].values,\n                input_data.loc[val_index, y_col_name_int].values,\n            )\n\n            # Train the model\n            self.curate_pipeline.fit(X_train, y_train)\n            classes_ = self.curate_pipeline.classes_.tolist()\n\n            # Make predictions on the validation set\n            y_pred_probs = self.curate_pipeline.predict_proba(X_val)\n\n            if self.calibration_method == \"calibrate_using_baseline\":\n                y_preds, y_pred_probs, label_cscores = self._calibrate_using_baseline(\n                    y_val,\n                    y_pred_probs,\n                    label_list=classes_,\n                    baseline_probs=baseline_probs,\n                )\n            else:\n                y_preds, y_pred_probs, label_cscores = self._no_calibration(\n                    y_val, y_pred_probs, label_list=classes_\n                )\n\n            predictions.extend(y_preds)\n            prediction_probabilities.extend(y_pred_probs)\n            label_correctness_scores.extend(label_cscores)\n            row_ids.extend(input_data.loc[val_index, row_id_col].values)\n\n        # Order dataframe according to `rowids``\n\n        row_id_df = pd.DataFrame()\n        row_id_df[row_id_col] = pd.Series(row_ids)\n        input_data = pd.merge(row_id_df, input_data, how=\"left\", on=row_id_col)\n\n        # Add results as columns\n        input_data[\"label_correctness_score\"] = pd.Series(label_correctness_scores)\n        input_data[\"predicted_label_int\"] = pd.Series(predictions)\n\n        input_data[\"prediction_probability\"] = pd.Series(prediction_probabilities)\n\n        logger.info(\"Identifying the correctly labelled samples..\")\n        input_data[\"is_label_correct\"] = input_data.progress_apply(\n            self._is_confident, axis=1\n        )\n\n        return dp._postprocess(\n            input_data,\n            display_cols=list(data_with_noisy_labels.columns) + self.result_col_list,\n            inv_label_mapping=inv_label_mapping,\n        )\n</code></pre>"},{"location":"api/crossval/#dqc.CrossValCurate.fit_transform","title":"<code>fit_transform(data_with_noisy_labels, X_col_name='text', y_col_name='label', **options)</code>","text":"<p>Fit CrossValCurate on the given input data</p> <p>Parameters:</p> Name Type Description Default <code>data_with_noisy_labels</code> <code>DataFrame</code> <p>Input data with corresponding noisy labels</p> required <code>X_col_name</code> <code>str</code> <p>Column to be used to extract input features.</p> <code>'text'</code> <code>y_col_name</code> <code>str</code> <p>Label column in the data.</p> <code>'label'</code> <p>Returns:</p> <pre><code>pd.DataFrame: Input Data samples with\n\n    1) 'predicted_label' - Predicted labels using CrossValCurate\n\n    2) 'prediction_probability' - Corresponding prediction probabilities\n\n    3) 'label_correctness_score' - Label correctness scores based on prediction probabilities\n\n    4) 'is_label_correct' - An indicator variable (True / False) classifying samples as correctly / incorrectly labelled.\n</code></pre> Source code in <code>dqc/crossval.py</code> <pre><code>@_exception_handler\ndef fit_transform(\n    self,\n    data_with_noisy_labels: pd.DataFrame,\n    X_col_name: str = \"text\",\n    y_col_name: str = \"label\",\n    **options,\n) -&gt; pd.DataFrame:\n    \"\"\"Fit CrossValCurate on the given input data\n\n    Args:\n        data_with_noisy_labels (pd.DataFrame): Input data with corresponding noisy labels\n        X_col_name (str): Column to be used to extract input features.\n        y_col_name (str): Label column in the data.\n    Returns:\n\n        pd.DataFrame: Input Data samples with \\n\n            1) 'predicted_label' - Predicted labels using CrossValCurate \\n\n            2) 'prediction_probability' - Corresponding prediction probabilities \\n\n            3) 'label_correctness_score' - Label correctness scores based on prediction probabilities \\n\n            4) 'is_label_correct' - An indicator variable (True / False) classifying samples as correctly / incorrectly labelled.\n    \"\"\"\n    _is_valid(data_with_noisy_labels, X_col_name, y_col_name)\n    n_splits = self.n_splits\n    options[\"num_samples\"] = len(data_with_noisy_labels)\n\n    # Make a copy of `data_with_noisy_labels` to prevent accidental modification\n    input_data = data_with_noisy_labels.copy()\n    logger.info(\"Pre-processing the data..\")\n    dp = _DataProcessor(\n        random_state=self.random_state,\n    )\n\n    input_data, row_id_col, y_col_name_int, inv_label_mapping = dp._preprocess(\n        input_data, y_col_name=y_col_name\n    )\n\n    # y_col_name_int needs to be accessed downstream\n    self.y_col_name_int = y_col_name_int\n\n    data_columns = [X_col_name, y_col_name_int]\n    logger.info(\n        f\"Building the curation pipeline with {n_splits}-fold cross validation..\"\n    )\n    self.curate_pipeline = _get_pipeline(\n        self.curate_feature_extractor, self.curate_model, **options\n    )\n\n    split_indices = _data_splitter(\n        input_data,\n        X_col_name=X_col_name,\n        y_col_name_int=y_col_name_int,\n        n_splits=self.n_splits,\n    )\n\n    # Lists to store predictions\n    predictions = []\n    prediction_probabilities = []\n    label_correctness_scores = []\n\n    # Keep track of shuffled data\n    row_ids = []\n\n    if self.calibration_method == \"calibrate_using_baseline\":\n        logger.info(\"Computing baseline predictions for each label..\")\n        baseline_probs = self._get_baselines(input_data[data_columns])\n\n    # Iterate through kfold splits\n    for train_index, val_index in tqdm(split_indices):\n        # Split the data\n        X_train, X_val = (\n            input_data.loc[train_index, X_col_name].values,\n            input_data.loc[val_index, X_col_name].values,\n        )\n        y_train, y_val = (\n            input_data.loc[train_index, y_col_name_int].values,\n            input_data.loc[val_index, y_col_name_int].values,\n        )\n\n        # Train the model\n        self.curate_pipeline.fit(X_train, y_train)\n        classes_ = self.curate_pipeline.classes_.tolist()\n\n        # Make predictions on the validation set\n        y_pred_probs = self.curate_pipeline.predict_proba(X_val)\n\n        if self.calibration_method == \"calibrate_using_baseline\":\n            y_preds, y_pred_probs, label_cscores = self._calibrate_using_baseline(\n                y_val,\n                y_pred_probs,\n                label_list=classes_,\n                baseline_probs=baseline_probs,\n            )\n        else:\n            y_preds, y_pred_probs, label_cscores = self._no_calibration(\n                y_val, y_pred_probs, label_list=classes_\n            )\n\n        predictions.extend(y_preds)\n        prediction_probabilities.extend(y_pred_probs)\n        label_correctness_scores.extend(label_cscores)\n        row_ids.extend(input_data.loc[val_index, row_id_col].values)\n\n    # Order dataframe according to `rowids``\n\n    row_id_df = pd.DataFrame()\n    row_id_df[row_id_col] = pd.Series(row_ids)\n    input_data = pd.merge(row_id_df, input_data, how=\"left\", on=row_id_col)\n\n    # Add results as columns\n    input_data[\"label_correctness_score\"] = pd.Series(label_correctness_scores)\n    input_data[\"predicted_label_int\"] = pd.Series(predictions)\n\n    input_data[\"prediction_probability\"] = pd.Series(prediction_probabilities)\n\n    logger.info(\"Identifying the correctly labelled samples..\")\n    input_data[\"is_label_correct\"] = input_data.progress_apply(\n        self._is_confident, axis=1\n    )\n\n    return dp._postprocess(\n        input_data,\n        display_cols=list(data_with_noisy_labels.columns) + self.result_col_list,\n        inv_label_mapping=inv_label_mapping,\n    )\n</code></pre>"},{"location":"api/llm/","title":"LLM based Curation","text":""},{"location":"api/llm/#llmcurate","title":"LLMCurate","text":"<p>             Bases: <code>BaseCurate</code></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AutoModelForCausalLM</code> <p>Instantiated LLM</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>Instantiated tokenizer corresponding to the <code>model</code></p> required <code>verbose</code> <code>bool</code> <p>Sets the verbosity level during execution. <code>True</code> indicates logging level INFO and <code>False</code> indicates logging level 'WARNING'. Defaults to False.</p> <code>False</code> <p>Examples:</p> <p>```python</p> <p>llmc = LLMCurate(model, tokenizer) ds = llmc.run(         data,         column_to_curate,         ds_column_mapping,         prompt_variants,         llm_response_cleaned_column_list,         answer_start_token,         answer_end_token,         batch_size,         max_new_tokens         ) <code>`` where *</code>model<code>and</code>tokenizer<code>are the instantiated LLM model and tokenizer objects respectively *</code>data<code>is a pandas dataframe containing samples with our target text for curation under column</code>column_to_curate<code>*</code>ds_column_mapping<code>is the dictionary mapping of entities used in the LLM prompt and the corresponding columns in</code>data<code>. For example,</code>ds_column_mapping={'INPUT' : 'input_column'}<code>would imply that text under</code>input_column<code>in</code>data<code>would be passed to the LLM in the format</code>\"[INPUT]row['input_column'][/INPUT]\"<code>for each</code>row<code>in</code>data<code>*</code>prompt_variants<code>is the list of LLM prompts to be used to curate</code>column_to_curate<code>and</code>llm_response_cleaned_column_list<code>is the corresponding list of column names to store the reference responses generated using each prompt *</code>answer_start_token<code>and</code>answer_end_token` are optional  text phrases representing the start and end of the answer respectively.</p> <p><code>ds</code> is a dataset object with the following additional features - 1. Feature for each column name in <code>llm_response_cleaned_column_list</code> 2. LLM Confidence score for each text in <code>column_to_curate</code></p> Source code in <code>dqc/llm.py</code> <pre><code>class LLMCurate(BaseCurate):\n    \"\"\"\n    Args:\n        model (AutoModelForCausalLM): Instantiated LLM\n        tokenizer (AutoTokenizer): Instantiated tokenizer corresponding to the `model`\n        verbose (bool, optional): Sets the verbosity level during execution. `True` indicates logging level INFO and `False` indicates logging level 'WARNING'. Defaults to False.\n\n    Examples:\n     ```python\n\n    llmc = LLMCurate(model, tokenizer)\n    ds = llmc.run(\n            data,\n            column_to_curate,\n            ds_column_mapping,\n            prompt_variants,\n            llm_response_cleaned_column_list,\n            answer_start_token,\n            answer_end_token,\n            batch_size,\n            max_new_tokens\n            )\n    ```\n    where\n    * `model` and `tokenizer` are the instantiated LLM model and tokenizer objects respectively\n    * `data` is a pandas dataframe containing samples with our target text for curation under column `column_to_curate`\n    * `ds_column_mapping` is the dictionary mapping of entities used in the LLM prompt and the corresponding columns in `data`. For example, `ds_column_mapping={'INPUT' : 'input_column'}` would imply that text under `input_column` in `data` would be passed to the LLM in the format `\"[INPUT]row['input_column'][/INPUT]\"` for each `row` in `data`\n    * `prompt_variants` is the list of LLM prompts to be used to curate `column_to_curate` and `llm_response_cleaned_column_list` is the corresponding list of column names to store the reference responses generated using each prompt\n    * `answer_start_token` and `answer_end_token` are optional  text phrases representing the start and end of the answer respectively.\n\n    `ds` is a dataset object with the following additional features -\n    1. Feature for each column name in `llm_response_cleaned_column_list`\n    2. LLM Confidence score for each text in `column_to_curate`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: AutoModelForCausalLM,\n        tokenizer: AutoTokenizer,\n        verbose: bool = False,\n        **options,\n    ):\n        super().__init__(**options)\n\n        _validate_init_params(model, tokenizer, verbose)\n        self.model = model\n        self.tokenizer = tokenizer\n        self.verbose = verbose\n        self._set_verbosity(verbose)\n\n        self.ds_ensemble = None\n\n    def __str__(self):\n        display_dict = self.__dict__.copy()\n\n        for key in list(display_dict.keys()):\n            if key in [\"ds_ensemble\"]:\n                ## Don't need to display these attributes\n                del display_dict[key]\n\n        return str(display_dict)\n\n    __repr__ = __str__\n\n    def _set_verbosity(self, verbose: bool):\n        \"\"\"Set logger level based on user input for parameter `verbose`\n\n        Args:\n            verbose (bool): Indicator for verbosity\n        \"\"\"\n        if verbose:\n            logger.set_level(\"INFO\")\n        else:\n            logger.set_level(\"WARNING\")\n\n    def fit_transform(self):\n        pass\n\n    def run(\n        self,\n        column_to_curate: str,\n        data: Union[pd.DataFrame, Dataset] = None,\n        ds_column_mapping: dict = {},\n        prompt_variants: List[str] = [\"\"],\n        skip_llm_inference: bool = False,\n        llm_response_cleaned_column_list: List[str] = [\"reference_prediction\"],\n        return_scores: bool = True,\n        answer_start_token: str = \"\",\n        answer_end_token: str = \"\",\n        scoring_params: dict = {\n            \"scoring_method\": \"exact_match\",\n            \"case_sensitive\": False,\n        },\n        **options,\n    ) -&gt; Dataset:\n        \"\"\"Run LLMCurate on the input data\n\n        Args:\n            column_to_curate (str): Column name in `data` with the text that needs to be curated\n            data (Union[pd.DataFrame, Dataset]): Input data for LLM based curation\n            ds_column_mapping (dict, optional): Mapping of entities to be used in the LLM prompt and the corresponding columns in the input data. Defaults to {}.\n            prompt_variants (List[str], optional): List of different LLM prompts to be used to curate the labels under `column_to_curate`. Defaults to [''].\n            skip_llm_inference (bool, optional): Indicator variable to prevent re-running LLM inference. Set to `True` if artifacts from the previous run of LLMCurate needs to be reused. Else `False`. Defaults to False.\n            llm_response_cleaned_column_list (list, optional): Names of the columns that will contain LLM predictions for each input prompt in `prompt_variants`. Defaults to ['reference_prediction'].\n            return_scores (bool, optional): Indicator variable set to `True` if label confidence scores are to be computed for each label under `column_to_curate`. Defaults to True.\n            answer_start_token (str, optional): Token that indicates the start of answer generation. Defaults to ''\n            answer_end_token (str, optional): Token that indicates the end of answer generation. Defaults to ''\n            scoring_params (dict, optional): Parameters related to util function `compute_selfensembling_confidence_score` to compute confidence scores of `column_to_curate`\n\n        Returns:\n            Dataset: Input dataset with reference responses. If `return_scores=True`, then input dataset with reference responses and confidence scores.\n        \"\"\"\n        if not skip_llm_inference:\n            empty_string_col_list = _validate_run_params(\n                data,\n                column_to_curate,\n                ds_column_mapping,\n                prompt_variants,\n                llm_response_cleaned_column_list,\n            )\n\n            if len(empty_string_col_list) &gt; 0:\n                logger.warning(\n                    \"Found empty string(s) in the input data under column(s) {empty_string_col_list}\"\n                )\n\n            logger.info(\n                f\"Running the LLM to generate the {len(prompt_variants)} reference responses using `prompt_variants`..\"\n            )\n            ds_ensemble = None\n\n            model = self.model\n            tokenizer = self.tokenizer\n\n            for index, prompt_template_prefix in enumerate(prompt_variants):\n                proposed_answer_col_name = llm_response_cleaned_column_list[index]\n                ds = run_LLM(\n                    data,\n                    model,\n                    tokenizer,\n                    ds_column_mapping=ds_column_mapping,\n                    prompt_template_prefix=prompt_template_prefix,\n                    answer_start_token=answer_start_token,\n                    answer_end_token=answer_end_token,\n                    llm_response_cleaned_col_name=proposed_answer_col_name,\n                    random_state=self.random_state,\n                    **options,\n                )\n\n                if not ds_ensemble:\n                    ds_ensemble = ds\n                else:\n                    ds_ensemble = ds_ensemble.add_column(\n                        proposed_answer_col_name, ds[proposed_answer_col_name]\n                    )\n            self.ds_ensemble = ds_ensemble\n\n        if return_scores:\n            if skip_llm_inference:\n                if (\n                    isinstance(data, pd.DataFrame)\n                    or ds_column_mapping\n                    or prompt_variants\n                ):\n                    logger.warning(\n                        \"Ignoring params `data`, `ds_column_mapping` and `prompt_variants` since `skip_llm_inference` is set to `True`\"\n                    )\n\n            _empty_ds_ensemble_handler(len(self.ds_ensemble) == 0, skip_llm_inference)\n\n            logger.info(\n                \"Computing confidence scores using the LLM reference responses..\"\n            )\n            self.ds_ensemble = self.ds_ensemble.map(\n                compute_selfensembling_confidence_score,\n                fn_kwargs={\n                    \"target_column\": column_to_curate,\n                    \"reference_column_list\": llm_response_cleaned_column_list,\n                    **scoring_params,\n                },\n            )\n\n        return self.ds_ensemble\n</code></pre>"},{"location":"api/llm/#dqc.LLMCurate.run","title":"<code>run(column_to_curate, data=None, ds_column_mapping={}, prompt_variants=[''], skip_llm_inference=False, llm_response_cleaned_column_list=['reference_prediction'], return_scores=True, answer_start_token='', answer_end_token='', scoring_params={'scoring_method': 'exact_match', 'case_sensitive': False}, **options)</code>","text":"<p>Run LLMCurate on the input data</p> <p>Parameters:</p> Name Type Description Default <code>column_to_curate</code> <code>str</code> <p>Column name in <code>data</code> with the text that needs to be curated</p> required <code>data</code> <code>Union[DataFrame, Dataset]</code> <p>Input data for LLM based curation</p> <code>None</code> <code>ds_column_mapping</code> <code>dict</code> <p>Mapping of entities to be used in the LLM prompt and the corresponding columns in the input data. Defaults to {}.</p> <code>{}</code> <code>prompt_variants</code> <code>List[str]</code> <p>List of different LLM prompts to be used to curate the labels under <code>column_to_curate</code>. Defaults to [''].</p> <code>['']</code> <code>skip_llm_inference</code> <code>bool</code> <p>Indicator variable to prevent re-running LLM inference. Set to <code>True</code> if artifacts from the previous run of LLMCurate needs to be reused. Else <code>False</code>. Defaults to False.</p> <code>False</code> <code>llm_response_cleaned_column_list</code> <code>list</code> <p>Names of the columns that will contain LLM predictions for each input prompt in <code>prompt_variants</code>. Defaults to ['reference_prediction'].</p> <code>['reference_prediction']</code> <code>return_scores</code> <code>bool</code> <p>Indicator variable set to <code>True</code> if label confidence scores are to be computed for each label under <code>column_to_curate</code>. Defaults to True.</p> <code>True</code> <code>answer_start_token</code> <code>str</code> <p>Token that indicates the start of answer generation. Defaults to ''</p> <code>''</code> <code>answer_end_token</code> <code>str</code> <p>Token that indicates the end of answer generation. Defaults to ''</p> <code>''</code> <code>scoring_params</code> <code>dict</code> <p>Parameters related to util function <code>compute_selfensembling_confidence_score</code> to compute confidence scores of <code>column_to_curate</code></p> <code>{'scoring_method': 'exact_match', 'case_sensitive': False}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Input dataset with reference responses. If <code>return_scores=True</code>, then input dataset with reference responses and confidence scores.</p> Source code in <code>dqc/llm.py</code> <pre><code>def run(\n    self,\n    column_to_curate: str,\n    data: Union[pd.DataFrame, Dataset] = None,\n    ds_column_mapping: dict = {},\n    prompt_variants: List[str] = [\"\"],\n    skip_llm_inference: bool = False,\n    llm_response_cleaned_column_list: List[str] = [\"reference_prediction\"],\n    return_scores: bool = True,\n    answer_start_token: str = \"\",\n    answer_end_token: str = \"\",\n    scoring_params: dict = {\n        \"scoring_method\": \"exact_match\",\n        \"case_sensitive\": False,\n    },\n    **options,\n) -&gt; Dataset:\n    \"\"\"Run LLMCurate on the input data\n\n    Args:\n        column_to_curate (str): Column name in `data` with the text that needs to be curated\n        data (Union[pd.DataFrame, Dataset]): Input data for LLM based curation\n        ds_column_mapping (dict, optional): Mapping of entities to be used in the LLM prompt and the corresponding columns in the input data. Defaults to {}.\n        prompt_variants (List[str], optional): List of different LLM prompts to be used to curate the labels under `column_to_curate`. Defaults to [''].\n        skip_llm_inference (bool, optional): Indicator variable to prevent re-running LLM inference. Set to `True` if artifacts from the previous run of LLMCurate needs to be reused. Else `False`. Defaults to False.\n        llm_response_cleaned_column_list (list, optional): Names of the columns that will contain LLM predictions for each input prompt in `prompt_variants`. Defaults to ['reference_prediction'].\n        return_scores (bool, optional): Indicator variable set to `True` if label confidence scores are to be computed for each label under `column_to_curate`. Defaults to True.\n        answer_start_token (str, optional): Token that indicates the start of answer generation. Defaults to ''\n        answer_end_token (str, optional): Token that indicates the end of answer generation. Defaults to ''\n        scoring_params (dict, optional): Parameters related to util function `compute_selfensembling_confidence_score` to compute confidence scores of `column_to_curate`\n\n    Returns:\n        Dataset: Input dataset with reference responses. If `return_scores=True`, then input dataset with reference responses and confidence scores.\n    \"\"\"\n    if not skip_llm_inference:\n        empty_string_col_list = _validate_run_params(\n            data,\n            column_to_curate,\n            ds_column_mapping,\n            prompt_variants,\n            llm_response_cleaned_column_list,\n        )\n\n        if len(empty_string_col_list) &gt; 0:\n            logger.warning(\n                \"Found empty string(s) in the input data under column(s) {empty_string_col_list}\"\n            )\n\n        logger.info(\n            f\"Running the LLM to generate the {len(prompt_variants)} reference responses using `prompt_variants`..\"\n        )\n        ds_ensemble = None\n\n        model = self.model\n        tokenizer = self.tokenizer\n\n        for index, prompt_template_prefix in enumerate(prompt_variants):\n            proposed_answer_col_name = llm_response_cleaned_column_list[index]\n            ds = run_LLM(\n                data,\n                model,\n                tokenizer,\n                ds_column_mapping=ds_column_mapping,\n                prompt_template_prefix=prompt_template_prefix,\n                answer_start_token=answer_start_token,\n                answer_end_token=answer_end_token,\n                llm_response_cleaned_col_name=proposed_answer_col_name,\n                random_state=self.random_state,\n                **options,\n            )\n\n            if not ds_ensemble:\n                ds_ensemble = ds\n            else:\n                ds_ensemble = ds_ensemble.add_column(\n                    proposed_answer_col_name, ds[proposed_answer_col_name]\n                )\n        self.ds_ensemble = ds_ensemble\n\n    if return_scores:\n        if skip_llm_inference:\n            if (\n                isinstance(data, pd.DataFrame)\n                or ds_column_mapping\n                or prompt_variants\n            ):\n                logger.warning(\n                    \"Ignoring params `data`, `ds_column_mapping` and `prompt_variants` since `skip_llm_inference` is set to `True`\"\n                )\n\n        _empty_ds_ensemble_handler(len(self.ds_ensemble) == 0, skip_llm_inference)\n\n        logger.info(\n            \"Computing confidence scores using the LLM reference responses..\"\n        )\n        self.ds_ensemble = self.ds_ensemble.map(\n            compute_selfensembling_confidence_score,\n            fn_kwargs={\n                \"target_column\": column_to_curate,\n                \"reference_column_list\": llm_response_cleaned_column_list,\n                **scoring_params,\n            },\n        )\n\n    return self.ds_ensemble\n</code></pre>"},{"location":"api/llm_utils/llm_utils/","title":"LLM Utils","text":""},{"location":"api/llm_utils/llm_utils/#build_llm_prompt","title":"build_LLM_prompt","text":"<p>Util function to build the LLM prompt from input text data</p> <p>Parameters:</p> Name Type Description Default <code>input_ds</code> <code>Dataset</code> <p>Input dataset containing text</p> required <code>ds_column_mapping</code> <code>dict</code> <p>Dictionary mapping prompt entities to dataset column names.</p> required <code>prompt_template_prefix</code> <code>Union[str, None]</code> <p>Text instruction to prepend to each transformed input text sample. Defaults to \"\".</p> <code>''</code> <code>answer_start_token</code> <code>str</code> <p>Token to append to the prompt to indicate start of the answer. Defaults to \"\"</p> <code>''</code> <code>llm_prompt_col_name</code> <code>str</code> <p>Name of the column for the built LLM prompts. Defaults to 'llm_prompt'</p> <code>'llm_prompt'</code> <p>Returns:     Dataset: Dataset with generated predictions.</p> Source code in <code>dqc/llm_utils/inference.py</code> <pre><code>def build_LLM_prompt(\n    input_ds: Dataset,\n    ds_column_mapping: dict,\n    prompt_template_prefix: str = \"\",\n    answer_start_token: str = \"\",\n    llm_prompt_col_name: str = \"llm_prompt\",\n) -&gt; Dataset:\n    \"\"\"Util function to build the LLM prompt from input text data\n\n    Args:\n        input_ds (Dataset): Input dataset containing text\n        ds_column_mapping (dict): Dictionary mapping prompt entities to dataset column names.\n        prompt_template_prefix (Union[str, None], optional): Text instruction to prepend to each transformed input text sample. Defaults to \"\".\n        answer_start_token (str, optional): Token to append to the prompt to indicate start of the answer. Defaults to \"\"\n        llm_prompt_col_name (str, optional): Name of the column for the built LLM prompts. Defaults to 'llm_prompt'\n    Returns:\n        Dataset: Dataset with generated predictions.\n    \"\"\"\n    if type(input_ds) == pd.DataFrame:\n        input_ds = Dataset.from_pandas(input_ds)\n\n    def _helper(\n        example: datasets.formatting.formatting.LazyBatch,\n        prompt_template_prefix: str,\n        ds_column_mapping: dict,\n        llm_prompt_col_name: str,\n    ) -&gt; dict:\n        llm_prompt = prompt_template_prefix\n        for entity_name, col_name in ds_column_mapping.items():\n            if col_name:\n                entity_value = example[col_name]\n                if type(entity_value) == list:\n                    entity_value = \"|| \".join(map(str, entity_value))\n                else:\n                    entity_value = str(entity_value)\n                llm_prompt += f\"[{entity_name}]{entity_value}[/{entity_name}]\"\n\n        if answer_start_token:\n            llm_prompt += answer_start_token\n\n        return {llm_prompt_col_name: llm_prompt}\n\n    input_ds = input_ds.map(\n        _helper,\n        fn_kwargs={\n            \"prompt_template_prefix\": prompt_template_prefix,\n            \"ds_column_mapping\": ds_column_mapping,\n            \"llm_prompt_col_name\": llm_prompt_col_name,\n        },\n    )\n    return input_ds\n</code></pre>"},{"location":"api/llm_utils/llm_utils/#infer_llm","title":"infer_LLM","text":"<p>Util function to run LLM inference</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AutoModelForCausalLM</code> <p>LLM artifact.</p> required <code>tokenizer</code> <code>Autotokenizer) </code> <p>LLM tokenizer object</p> required <code>input_ds</code> <code>Dataset</code> <p>Input dataset containing text prompts.</p> required <code>llm_prompt_col_name</code> <code>str</code> <p>Name of the column containing text prompts. Defaults to 'llm_prompt'.</p> <code>'llm_prompt'</code> <code>llm_response_raw_col_name</code> <code>str</code> <p>Name of the column containing prediction. Defaults to 'llm_response'.</p> <code>'llm_response'</code> <p>Returns:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>Dataset with generated predictions.</p> Source code in <code>dqc/llm_utils/inference.py</code> <pre><code>def infer_LLM(\n    model: AutoModelForCausalLM,\n    tokenizer: AutoTokenizer,\n    input_ds: Dataset,\n    llm_prompt_col_name: str = \"llm_prompt\",\n    llm_response_raw_col_name: str = \"llm_response\",\n    **options,\n) -&gt; Dataset:\n    \"\"\"\n    Util function to run LLM inference\n\n    Args:\n        model (AutoModelForCausalLM): LLM artifact.\n        tokenizer (Autotokenizer) : LLM tokenizer object\n        input_ds (Dataset): Input dataset containing text prompts.\n        llm_prompt_col_name (str, optional): Name of the column containing text prompts. Defaults to 'llm_prompt'.\n        llm_response_raw_col_name (str, optional): Name of the column containing prediction. Defaults to 'llm_response'.\n\n    Returns:\n        dataset: Dataset with generated predictions.\n    \"\"\"\n    if options[\"random_state\"]:\n        _set_seed(options[\"random_state\"])\n        del options[\"random_state\"]\n\n    text_generator = pipeline(\n        \"text-generation\", model=model, tokenizer=tokenizer, truncation=False, **options\n    )\n    text_generator.tokenizer.pad_token_id = model.config.eos_token_id\n\n    batch_size = options[\"batch_size\"] if \"batch_size\" in options else 8\n\n    input_ds = input_ds.map(\n        _generate_predictions,\n        fn_kwargs={\n            \"generator\": text_generator,\n            \"llm_prompt_col_name\": llm_prompt_col_name,\n            \"llm_response_raw_col_name\": llm_response_raw_col_name,\n            **options,\n        },\n        batched=True,\n        batch_size=batch_size,\n    )\n\n    return input_ds\n</code></pre>"},{"location":"api/llm_utils/llm_utils/#run_llm","title":"run_LLM","text":"<p>Run end-to-end LLM inference (from pre-processing input data to post-processing the predictions) and return the computed performance metrics on input validation data</p> <p>Parameters:</p> Name Type Description Default <code>val_data</code> <code>Union[DataFrame, Dataset]</code> <p>Validation data with labels</p> required <code>model</code> <code>AutoModelForCausalLM</code> <p>LLM artifact.</p> required <code>tokenizer</code> <code>Autotokenizer) </code> <p>LLM tokenizer object</p> required <code>ds_column_mapping</code> <code>dict</code> <p>Dictionary mapping prompt entities to dataset column names.</p> required <code>prompt_template_prefix</code> <code>Union[str, None]</code> <p>Text instruction to prepend to each transformed input text sample. Defaults to \"\".</p> <code>''</code> <code>llm_prompt_col_name</code> <code>str</code> <p>Name of the column with the built LLM prompts. Defaults to 'llm_prompt'</p> <code>'llm_prompt'</code> <code>llm_response_raw_col_name</code> <code>str</code> <p>Name of the column containing prediction. Defaults to 'llm_response'.</p> <code>'llm_response'</code> <code>llm_response_cleaned_col_name</code> <code>str</code> <p>Name of the column containing the final post processed result. Defaults to 'llm_response_cleaned'</p> <code>'llm_response_cleaned'</code> <code>answer_start_token</code> <code>str</code> <p>Token that indicates the start of answer generation. Defaults to ''</p> <code>''</code> <code>answer_end_token</code> <code>str</code> <p>Token that indicates the end of answer generation. Defaults to ''</p> <code>''</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing F1 score.</p> Source code in <code>dqc/llm_utils/inference.py</code> <pre><code>def run_LLM(\n    val_data: Union[pd.DataFrame, Dataset],\n    model: AutoModelForCausalLM,\n    tokenizer: AutoTokenizer,\n    ds_column_mapping: dict,\n    prompt_template_prefix: Union[str, None] = \"\",\n    llm_prompt_col_name: str = \"llm_prompt\",\n    llm_response_raw_col_name: str = \"llm_response\",\n    llm_response_cleaned_col_name: str = \"llm_response_cleaned\",\n    answer_start_token: str = \"\",\n    answer_end_token: str = \"\",\n    **options,\n) -&gt; dict:\n    \"\"\"Run end-to-end LLM inference (from pre-processing input data to post-processing the predictions) and return the computed performance metrics on input validation data\n\n    Args:\n        val_data (Union[pd.DataFrame, Dataset]): Validation data with labels\n        model (AutoModelForCausalLM): LLM artifact.\n        tokenizer (Autotokenizer) : LLM tokenizer object\n        ds_column_mapping (dict): Dictionary mapping prompt entities to dataset column names.\n        prompt_template_prefix (Union[str, None], optional): Text instruction to prepend to each transformed input text sample. Defaults to \"\".\n        llm_prompt_col_name (str, optional): Name of the column with the built LLM prompts. Defaults to 'llm_prompt'\n        llm_response_raw_col_name (str, optional): Name of the column containing prediction. Defaults to 'llm_response'.\n        llm_response_cleaned_col_name (str, optional): Name of the column containing the final post processed result. Defaults to 'llm_response_cleaned'\n        answer_start_token (str, optional): Token that indicates the start of answer generation. Defaults to ''\n        answer_end_token (str, optional): Token that indicates the end of answer generation. Defaults to ''\n\n    Returns:\n        dict: A dictionary containing F1 score.\n    \"\"\"\n    predicted_label_list = []\n\n    val_ds = build_LLM_prompt(\n        val_data,\n        ds_column_mapping=ds_column_mapping,\n        prompt_template_prefix=prompt_template_prefix,\n        answer_start_token=answer_start_token,\n        llm_prompt_col_name=llm_prompt_col_name,\n    )\n\n    val_ds_with_pred = infer_LLM(\n        model,\n        tokenizer,\n        val_ds,\n        llm_prompt_col_name=llm_prompt_col_name,\n        llm_response_raw_col_name=llm_response_raw_col_name,\n        **options,\n    )\n\n    val_ds_with_pred = val_ds_with_pred.map(\n        _postprocess,\n        fn_kwargs={\n            \"llm_prompt_col_name\": llm_prompt_col_name,\n            \"llm_response_raw_col_name\": llm_response_raw_col_name,\n            \"llm_response_cleaned_col_name\": llm_response_cleaned_col_name,\n            \"answer_end_token\": answer_end_token,\n        },\n    )\n\n    return val_ds_with_pred\n</code></pre>"},{"location":"api/llm_utils/llm_utils/#compute_selfensembling_confidence_score","title":"compute_selfensembling_confidence_score","text":"<p>Util function to compute confidence score of a given target text using LLM generated reference texts.</p> <p>Parameters:</p> Name Type Description Default <code>example</code> <code>LazyRow</code> <p>A row of data from a dataset containing the target and reference texts.</p> required <code>target_column</code> <code>str</code> <p>Name of the column containing the target text for estimation of confidence score.</p> required <code>reference_column_list</code> <code>List[str]</code> <p>Names of the columns containing the reference texts to be compared with the target text.</p> required <code>scoring_method</code> <code>Union[Callable[[str, str], float], str]</code> <p>A function or the string 'exact_match' to compute the confidence score. Defaults to 'exact_match'.</p> <code>'exact_match'</code> <code>case_sensitive</code> <code>bool</code> <p><code>True</code> if string comparisons need to be case aware. Else <code>False</code>. Defaults to <code>False</code></p> <code>False</code> <p>Raises:     ValueError: If <code>scoring_method</code> is neither 'exact_match' nor a valid callable function</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Score between 0 and 1 quantifying the confidence score for the target text</p> Source code in <code>dqc/llm_utils/compute_confidence_score.py</code> <pre><code>def compute_selfensembling_confidence_score(\n    example: datasets.formatting.formatting.LazyRow,\n    target_column: str,\n    reference_column_list: List[str],\n    scoring_method: Union[Callable[[str, str], float], str] = \"exact_match\",\n    case_sensitive: bool = False,\n    **options,\n) -&gt; float:\n    \"\"\"Util function to compute confidence score of a given target text using LLM generated reference texts.\n\n    Args:\n        example (datasets.formatting.formatting.LazyRow): A row of data from a dataset containing the target and reference texts.\n        target_column (str): Name of the column containing the target text for estimation of confidence score.\n        reference_column_list (List[str]): Names of the columns containing the reference texts to be compared with the target text.\n        scoring_method (Union[Callable[[str, str], float], str], optional): A function or the string 'exact_match' to compute the confidence score. Defaults to 'exact_match'.\n        case_sensitive (bool, optional): `True` if string comparisons need to be case aware. Else `False`. Defaults to `False`\n    Raises:\n        ValueError: If `scoring_method` is neither 'exact_match' nor a valid callable function\n\n    Returns:\n        float: Score between 0 and 1 quantifying the confidence score for the target text\n    \"\"\"\n    if not callable(scoring_method) and scoring_method != \"exact_match\":\n        raise ValueError(\n            \"Parameter `scoring_method` must be 'exact_match' or a valid callable that measures string similarity\"\n        )\n\n    reference_text_list = []\n    result_dict = {}\n    score = 0\n\n    for col in reference_column_list:\n        reference_text_list.append(example[col])\n\n    target_text = example[target_column]\n\n    if not case_sensitive:\n        target_text = target_text.lower()\n        reference_text_list = [text.lower() for text in reference_text_list]\n\n    if scoring_method == \"exact_match\":\n        score = _compute_exact_match_score(target_text, reference_text_list)\n    else:\n        score = _compute_custom_match_score(\n            target_text, reference_text_list, scoring_method\n        )\n\n    return {\"confidence_score\": score}\n</code></pre>"},{"location":"api/utils/utils/","title":"Utils","text":""},{"location":"api/utils/utils/#add_asymmetric_noise","title":"add_asymmetric_noise","text":"<p>Util function to add asymmetric noise to labels for simulation of noisy label scenarios.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Series</code> <p>Input pandas series with integer values             ranging from 0 to n - 1.</p> required <code>noise_prob</code> <code>float</code> <p>Probability of adding noise to each value.</p> required <code>random_state</code> <code>Union[RandomState, None]</code> <p>Random seed for reproducibility</p> <code>42</code> <p>Returns:     pd.Series: Series with asymmetric noise added to it.     float: Normalized quantification of pairwise disagreement between <code>labels</code> and <code>noisy_labels</code> for parity check</p> Source code in <code>dqc/utils/noise_utils.py</code> <pre><code>def add_asymmetric_noise(\n    labels: pd.Series,\n    noise_prob: float,\n    random_state: Union[RandomState, None] = 42,\n) -&gt; Tuple[pd.Series, float]:\n    \"\"\"\n    Util function to add asymmetric noise to labels\n    for simulation of noisy label scenarios.\n\n    Args:\n        labels (pd.Series): Input pandas series with integer values\n                        ranging from 0 to n - 1.\n        noise_prob (float): Probability of adding noise to each value.\n        random_state (Union[RandomState, None]): Random seed for reproducibility\n    Returns:\n        pd.Series: Series with asymmetric noise added to it.\n        float: Normalized quantification of pairwise disagreement between `labels` and `noisy_labels` for parity check\n    \"\"\"\n    # Set seed\n    np.random.seed(random_state)\n\n    # Avoid modifying the original data\n    noisy_labels = labels.copy()\n\n    # Build a replacement dictionary\n    unique_labels = list(set(noisy_labels))\n    replacement_dict = {\n        label: [candidate for candidate in unique_labels if candidate != label]\n        for label in unique_labels\n    }\n\n    # Determine the number of samples to modify based on the noise probability\n    num_samples = min(len(noisy_labels), int(len(noisy_labels) * noise_prob + 1))\n\n    # Sample random indices from the labels to introduce noise\n    target_indices = np.random.choice(len(noisy_labels), num_samples, replace=False)\n\n    for idx in target_indices:\n        # Introduce noise\n        noisy_labels[idx] = np.random.choice(replacement_dict[noisy_labels[idx]])\n\n    # Parity check\n    num_mismatches = sum(\n        [\n            label != noisy_label\n            for label, noisy_label in zip(labels.values, noisy_labels.values)\n        ]\n    )\n    observed_noise_ratio = num_mismatches / len(noisy_labels)\n\n    return noisy_labels, observed_noise_ratio\n</code></pre>"},{"location":"api/utils/utils/#show_versions","title":"show_versions","text":"<p>Print useful debugging information</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary object containing system information</p> Source code in <code>dqc/version.py</code> <pre><code>def show_versions():\n    \"\"\"Print useful debugging information\n\n    Returns:\n        dict: Dictionary object containing system information\n    \"\"\"\n    versions = {\n        \"os_type\": platform.system(),\n        \"os_version\": platform.release(),\n        \"python_version\": platform.python_version(),\n        \"dqc-toolkit\": __version__,\n        \"transformers\": transformers.__version__,\n        \"sentence_transformers\": sentence_transformers.__version__,\n        \"accelerate\": accelerate.__version__,\n        \"scikit-learn\": sklearn.__version__,\n    }\n\n    return versions\n</code></pre>"}]}