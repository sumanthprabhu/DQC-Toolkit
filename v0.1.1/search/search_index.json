{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DQC toolkit","text":"<p>DQC Toolkit is a Python library and framework designed with the goal to facilitate improvement of Machine Learning models by identifying and mitigating label errors in training dataset. Currently, DQC toolkit offers <code>CrossValCurate</code> for curation of text classification datasets (binary / multi-class) using cross validation based label error detection / correction.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation of DQC-toolkit can be done as shown below <pre><code>pip install dqc-toolkit\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Assuming your text classification data is stored as a pandas dataframe <code>data</code>, with each sample represented by the <code>text</code> column and its corresponding noisy label represented by the <code>label</code> column,  here is how you use <code>CrossValCurate</code> - </p> <p><pre><code>from dqc import CrossValCurate\n\ncvc = CrossValCurate()\ndata_curated = cvc.fit_transform(data[['text', 'label']])\n</code></pre> The result stored in <code>data_curated</code> is a pandas dataframe similar to <code>data</code> with the following columns - <pre><code>&gt;&gt;&gt; data_curated.columns\n['text', 'label', 'label_correctness_score', 'is_label_correct', 'predicted_label', 'prediction_probability']\n</code></pre></p> <ul> <li><code>'label_correctness_score'</code> represents a normalized score quantifying the correctness of <code>'label'</code>. </li> <li><code>'is_label_correct'</code> is a boolean flag indicating whether the given <code>'label'</code> is correct (<code>True</code>) or incorrect (<code>False</code>). </li> <li><code>'predicted_label'</code> and <code>'prediction_probability'</code> represent the curation model's prediction and the corresponding probability score. </li> </ul> <p>For more details regarding different hyperparameters available in <code>CrossValCurate</code>, please refer to the API documentation.</p>"},{"location":"api/crossval/","title":"Cross Validation based Curation","text":""},{"location":"api/crossval/#basecurate","title":"BaseCurate","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for data curation to compute label correctness scores   and identify reliable labelled samples</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Random seed for                     reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>dqc/base.py</code> <pre><code>class BaseCurate(ABC):\n    \"\"\"Base class for data curation to compute label correctness scores\n      and identify reliable labelled samples\n\n    Args:\n        random_state (RandomState, optional): Random seed for\n                                reproducibility. Defaults to 42.\n    \"\"\"\n\n    def __init__(\n        self,\n        random_state: RandomState = 42,\n        **options,\n    ):\n        self.random_state = random_state\n\n    @abstractmethod\n    def fit_transform(self): ...\n</code></pre>"},{"location":"api/crossval/#crossvalcurate","title":"CrossValCurate","text":"<p>             Bases: <code>BaseCurate</code></p> <p>Parameters:</p> Name Type Description Default <code>curate_feature_extractor</code> <code>Union[str, TfidfVectorizer, CountVectorizer, SentenceTransformer]</code> <p>Feature extraction method to be used during curation. Accepts string values 'TfidfVectorizer', 'CountVectorizer' which map to <code>sklearn.feature_extraction.text.TfidfVectorizer</code> and <code>sklearn.feature_extraction.text.CountVectorizer</code> objects respectively. Also accepts explicit instances of <code>sklearn.feature_extraction.text.TfidfVectorizer</code> / <code>sklearn.feature_extraction.text.CountVectorizer</code>.  Additionally, supports <code>SentenceTransformer</code> model object or a string matching the name of a <code>SentenceTransformer</code> model hosted on HuggingFace Model Hub. Defaults to 'TfidfVectorizer'.</p> <code>'TfidfVectorizer'</code> <code>curate_model</code> <code>str</code> <p>Machine learning model that is trained with <code>curate_feature_extractor</code> based features during curation. Accepts any instance of scikit-learn classifier that implement <code>predict_proba()</code> method. Defaults to <code>sklearn.linear_model.LogisticRegression</code>.</p> <code>LogisticRegression()</code> <code>calibration_method</code> <code>Union[str, None]</code> <p>Approach to be used for calibration of <code>curate_model</code> predictions. Defaults to 'calibrate_using_baseline'.</p> <code>'calibrate_using_baseline'</code> <code>correctness_threshold</code> <code>float</code> <p>Minimum prediction probability using <code>curate_model</code> to consider the corresponding sample as 'correctly labelled'. Defaults to 0.0.</p> <code>0.0</code> <code>n_splits</code> <code>int</code> <p>Number of splits to use when running cross-validation based curation.</p> <code>5</code> <code>verbose</code> <code>bool</code> <p>Sets the verbosity level during execution. <code>True</code> indicates logging level INFO and <code>False</code> indicates logging level 'ERROR'.</p> <code>False</code> <p>Examples: Assuming <code>data</code> is a pandas dataframe containing samples with noisy labels, here is how you would use <code>CrossValCurate</code> - <pre><code>from dqc import CrossValCurate\n\ncvc = CrossValCurate()\ndata_curated = cvc.fit_transform(data[['text', 'label']])\n</code></pre></p> <p><code>data_curated</code> is a pandas dataframe similar to <code>data</code> with the following columns - <pre><code>&gt;&gt;&gt; data_curated.columns\n['text', 'label', 'label_correctness_score', 'is_label_correct', 'predicted_label', 'prediction_probability']\n</code></pre></p> <p><code>'label_correctness_score'</code> represents a normalized score quantifying the correctness of <code>'label'</code>. </p> <p><code>'is_label_correct'</code> is a boolean flag indicating whether the given <code>'label'</code> is correct (<code>True</code>) or incorrect (<code>False</code>). </p> <p><code>'predicted_label'</code> and <code>'prediction_probability'</code> represent the curation model's prediction and the corresponding probability score.</p> Source code in <code>dqc/crossval.py</code> <pre><code>class CrossValCurate(BaseCurate):\n    \"\"\"\n    Args:\n        curate_feature_extractor (Union[str, TfidfVectorizer, CountVectorizer, SentenceTransformer], optional): Feature extraction method to be used during curation. Accepts string values 'TfidfVectorizer', 'CountVectorizer' which map to `sklearn.feature_extraction.text.TfidfVectorizer` and `sklearn.feature_extraction.text.CountVectorizer` objects respectively. Also accepts explicit instances of `sklearn.feature_extraction.text.TfidfVectorizer` / `sklearn.feature_extraction.text.CountVectorizer`.  Additionally, supports `SentenceTransformer` model object or a string matching the name of a `SentenceTransformer` model hosted on HuggingFace Model Hub. Defaults to 'TfidfVectorizer'.\n        curate_model (str, optional): Machine learning model that is trained with `curate_feature_extractor` based features during curation. Accepts any instance of scikit-learn classifier that implement `predict_proba()` method. Defaults to `sklearn.linear_model.LogisticRegression`.\n        calibration_method (Union[str, None], optional): Approach to be used for calibration of `curate_model` predictions. Defaults to 'calibrate_using_baseline'.\n        correctness_threshold (float, optional): Minimum prediction probability using `curate_model` to consider the corresponding sample as 'correctly labelled'. Defaults to 0.0.\n        n_splits (int, optional): Number of splits to use when running cross-validation based curation.\n        verbose (bool, optional): Sets the verbosity level during execution. `True` indicates logging level INFO and `False` indicates logging level 'ERROR'.\n\n    Examples:\n    Assuming `data` is a pandas dataframe containing samples with noisy labels, here is how you would use `CrossValCurate` -\n    ```python\n\n    from dqc import CrossValCurate\n\n    cvc = CrossValCurate()\n    data_curated = cvc.fit_transform(data[['text', 'label']])\n    ```\n\n    `data_curated` is a pandas dataframe similar to `data` with the following columns -\n    ```python\n    &gt;&gt;&gt; data_curated.columns\n    ['text', 'label', 'label_correctness_score', 'is_label_correct', 'predicted_label', 'prediction_probability']\n    ```\n\n    `'label_correctness_score'` represents a normalized score quantifying the correctness of `'label'`. \\n\n    `'is_label_correct'` is a boolean flag indicating whether the given `'label'` is correct (`True`) or incorrect (`False`). \\n\n    `'predicted_label'` and `'prediction_probability'` represent the curation model's prediction and the corresponding probability score.\n    \"\"\"\n\n    def __init__(\n        self,\n        curate_feature_extractor: Union[\n            str, TfidfVectorizer, CountVectorizer, SentenceTransformerVectorizer\n        ] = \"TfidfVectorizer\",\n        curate_model: Union[str, ClassifierMixin] = LogisticRegression(),\n        calibration_method: Union[str, None] = \"calibrate_using_baseline\",\n        correctness_threshold: float = 0.0,\n        n_splits: int = 5,\n        verbose: bool = False,\n        **options,\n    ):\n        super().__init__(**options)\n\n        self.curate_feature_extractor, self.curate_model, self.calibration_method = (\n            _fetch_curation_artifacts(\n                curate_feature_extractor, curate_model, calibration_method\n            )\n        )\n\n        self.correctness_threshold = correctness_threshold\n        self.n_splits = n_splits\n        self.verbose = verbose\n        self._set_verbosity(verbose)\n\n        self.curate_pipeline = None\n        self.scaler = None\n        self.y_col_name_int = None\n        self.result_col_list = [\n            \"label_correctness_score\",\n            \"is_label_correct\",\n            \"predicted_label\",\n            \"prediction_probability\",\n        ]\n\n    def __str__(self):\n        display_dict = self.__dict__.copy()\n        for key in list(display_dict.keys()):\n            if key in [\n                \"curate_pipeline\",\n                \"scaler\",\n                \"y_col_name_int\",\n                \"result_col_list\",\n            ]:\n                ## Don't need to display these attributes\n                del display_dict[key]\n\n        return str(display_dict)\n\n    __repr__ = __str__\n\n    def _is_confident(self, row: pd.Series) -&gt; bool:\n        \"\"\"Return a boolean variable indicating whether we are confident\n           about the correctness of the label assigned to a given data sample\n\n        Args:\n            row (pd.Series): data sample whose label correctness need to be evaluated\n\n        Returns:\n            bool: `True` if we are confident that the label assigned is correct\n                else `False`\n        \"\"\"\n        threshold = self.correctness_threshold\n        if (row[\"predicted_label_int\"] == row[self.y_col_name_int]) and (\n            row[\"label_correctness_score\"] &gt;= threshold\n        ):\n            return True\n        return False\n\n    def _set_verbosity(self, verbose: bool):\n        \"\"\"Set logger level based on user input for parameter `verbose`\n\n        Args:\n            verbose (bool): Indicator for verbosity\n        \"\"\"\n        if verbose:\n            logger.set_level(\"INFO\")\n        else:\n            logger.set_level(\"WARNING\")\n\n    def _no_calibration(\n        self,\n        input_labels: List[Union[int, str]],\n        pred_prob_matrix: np.ndarray,\n        label_list: List[Union[int, str]],\n    ) -&gt; Tuple[List[Union[int, str]], List[float]]:\n        \"\"\"Returns predictions, corresponding probabilities and label correctness\n        scores without any calibration\n\n        Args:\n            input_labels (List[Union[int, str]]): _description_\n            pred_prob_matrix (np.ndarray): _description_\n            label_list (List[Union[int, str]]): _description_\n\n        Returns:\n            Tuple[List[Union[int, str]], List[float]]: _description_\n        \"\"\"\n        pred_probs = np.max(pred_prob_matrix, axis=1).tolist()\n        preds = [label_list[index] for index in np.argmax(pred_prob_matrix, axis=1)]\n        label_correctness_scores = [\n            pred_prob_matrix[row_index, label_list.index(label)]\n            for row_index, label in enumerate(input_labels)\n        ]\n\n        return preds, pred_probs, label_correctness_scores\n\n    def _get_baselines(self, input_data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Computes the baseline prediction probabilities using\n          input label distribution\n\n        Args:\n            input_data (pd.DataFrame): Input data with\n                                            corresponding noisy labels\n\n        Returns:\n            pd.DataFrame: Labels and corresponding probabilities\n        \"\"\"\n        thresholds_df = (input_data[self.y_col_name_int].value_counts(1)).reset_index()\n        thresholds_df.columns = [self.y_col_name_int, \"probability\"]\n        return thresholds_df\n\n    def _calibrate_using_baseline(\n        self,\n        input_labels: List[Union[int, str]],\n        pred_prob_matrix: np.ndarray,\n        label_list: List[Union[int, str]],\n        baseline_probs: pd.DataFrame,\n    ) -&gt; Tuple[List[Union[int, str]], List[float], List[float]]:\n        \"\"\"Calibrate the predicted probabilities using baseline probabilities and\n        returns the modified predictions, corresponding probabilities and label\n        correctness scores\n\n        Args:\n            input_labels (List[Union[int, str]]): Noisy labels from data\n            pred_prob_matrix (np.ndarray): Predicted probabilities per label\n                                for each sample in the data\n            label_list (List[Union[int, str]]): Ordered list of labels where\n                                ordering matches `pred_prob_matrix`\n            baseline_probs (pd.DataFrame): Prediction probabilities computed\n                                using input label distribution\n\n        Returns:\n            Tuple[List[Union[int, str]], List[float], List[float]]:\n            Returns the following\n            'calibrated_prediction' : Label predictions post calibration\n            'calibrated_probabilities' : Normalized scores corresponding\n                                    to 'calibrated_prediction'\n            'label_correctness_score' : Normalized scores corresponding\n                                     to 'label' (provided as input to\n                                    `self.fit_transform`)\n        \"\"\"\n\n        label_list_df = pd.DataFrame({\"label_list\": label_list})\n        baseline_probs = pd.merge(\n            label_list_df,\n            baseline_probs,\n            left_on=\"label_list\",\n            right_on=self.y_col_name_int,\n            how=\"inner\",\n        ).reset_index(drop=True)\n\n        baseline_probs.drop(\"label_list\", axis=1, inplace=True)\n\n        prob_array = baseline_probs[\"probability\"].values\n\n        # Calibrate prediction probabilities using baseline probabilities\n        pred_prob_matrix = (pred_prob_matrix - prob_array) / prob_array\n\n        pred_prob_matrix = normalize(pred_prob_matrix, norm=\"l2\")\n\n        calibrated_predictions = [\n            label_list[index] for index in np.argmax(pred_prob_matrix, axis=1)\n        ]\n        calibrated_probabilities = np.max(pred_prob_matrix, axis=1).tolist()\n        label_correctness_scores = [\n            pred_prob_matrix[row_index, label_list.index(label)]\n            for row_index, label in enumerate(input_labels)\n        ]\n\n        return (\n            calibrated_predictions,\n            calibrated_probabilities,\n            label_correctness_scores,\n        )\n\n    @_exception_handler\n    def fit_transform(\n        self,\n        data_with_noisy_labels: pd.DataFrame,\n        X_col_name: str = \"text\",\n        y_col_name: str = \"label\",\n        **options,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fit CrossValCurate on the given input data\n\n        Args:\n            data_with_noisy_labels (pd.DataFrame): Input data with corresponding noisy labels\n            X_col_name (str): Column to be used to extract input features.\n            y_col_name (str): Label column in the data.\n        Returns:\n\n            pd.DataFrame: Input Data samples with \\n\n                1) 'predicted_label' - Predicted labels using CrossValCurate \\n\n                2) 'prediction_probability' - Corresponding prediction probabilities \\n\n                3) 'label_correctness_score' - Label correctness scores based on prediction probabilities \\n\n                4) 'is_label_correct' - An indicator variable (True / False) classifying samples as correctly / incorrectly labelled.\n        \"\"\"\n        _is_valid(data_with_noisy_labels, X_col_name, y_col_name)\n        n_splits = self.n_splits\n        options[\"num_samples\"] = len(data_with_noisy_labels)\n\n        # Make a copy of `data_with_noisy_labels` to prevent accidental modification\n        input_data = data_with_noisy_labels.copy()\n        logger.info(\"Pre-processing the data..\")\n        dp = _DataProcessor(\n            random_state=self.random_state,\n        )\n\n        input_data, row_id_col, y_col_name_int, inv_label_mapping = dp._preprocess(\n            input_data, y_col_name=y_col_name\n        )\n\n        # y_col_name_int needs to be accessed downstream\n        self.y_col_name_int = y_col_name_int\n\n        data_columns = [X_col_name, y_col_name_int]\n        logger.info(\n            f\"Building the curation pipeline with {n_splits}-fold cross validation..\"\n        )\n        self.curate_pipeline = _get_pipeline(\n            self.curate_feature_extractor, self.curate_model, **options\n        )\n\n        split_indices = _data_splitter(\n            input_data,\n            X_col_name=X_col_name,\n            y_col_name_int=y_col_name_int,\n            n_splits=self.n_splits,\n        )\n\n        # Lists to store predictions\n        predictions = []\n        prediction_probabilities = []\n        label_correctness_scores = []\n\n        # Keep track of shuffled data\n        row_ids = []\n\n        if self.calibration_method == \"calibrate_using_baseline\":\n            logger.info(\"Computing baseline predictions for each label..\")\n            baseline_probs = self._get_baselines(input_data[data_columns])\n\n        # Iterate through kfold splits\n        for train_index, val_index in tqdm(split_indices):\n            # Split the data\n            X_train, X_val = (\n                input_data.loc[train_index, X_col_name].values,\n                input_data.loc[val_index, X_col_name].values,\n            )\n            y_train, y_val = (\n                input_data.loc[train_index, y_col_name_int].values,\n                input_data.loc[val_index, y_col_name_int].values,\n            )\n\n            # Train the model\n            self.curate_pipeline.fit(X_train, y_train)\n            classes_ = self.curate_pipeline.classes_.tolist()\n\n            # Make predictions on the validation set\n            y_pred_probs = self.curate_pipeline.predict_proba(X_val)\n\n            if self.calibration_method == \"calibrate_using_baseline\":\n                y_preds, y_pred_probs, label_cscores = self._calibrate_using_baseline(\n                    y_val,\n                    y_pred_probs,\n                    label_list=classes_,\n                    baseline_probs=baseline_probs,\n                )\n            else:\n                y_preds, y_pred_probs, label_cscores = self._no_calibration(\n                    y_val, y_pred_probs, label_list=classes_\n                )\n\n            predictions.extend(y_preds)\n            prediction_probabilities.extend(y_pred_probs)\n            label_correctness_scores.extend(label_cscores)\n            row_ids.extend(input_data.loc[val_index, row_id_col].values)\n\n        # Order dataframe according to `rowids``\n\n        row_id_df = pd.DataFrame()\n        row_id_df[row_id_col] = pd.Series(row_ids)\n        input_data = pd.merge(row_id_df, input_data, how=\"left\", on=row_id_col)\n\n        # Add results as columns\n        input_data[\"label_correctness_score\"] = pd.Series(label_correctness_scores)\n        input_data[\"predicted_label_int\"] = pd.Series(predictions)\n\n        input_data[\"prediction_probability\"] = pd.Series(prediction_probabilities)\n\n        logger.info(\"Identifying the correctly labelled samples..\")\n        input_data[\"is_label_correct\"] = input_data.progress_apply(\n            self._is_confident, axis=1\n        )\n\n        return dp._postprocess(\n            input_data,\n            display_cols=list(data_with_noisy_labels.columns) + self.result_col_list,\n            inv_label_mapping=inv_label_mapping,\n        )\n</code></pre>"},{"location":"api/crossval/#dqc.CrossValCurate.fit_transform","title":"<code>fit_transform(data_with_noisy_labels, X_col_name='text', y_col_name='label', **options)</code>","text":"<p>Fit CrossValCurate on the given input data</p> <p>Parameters:</p> Name Type Description Default <code>data_with_noisy_labels</code> <code>DataFrame</code> <p>Input data with corresponding noisy labels</p> required <code>X_col_name</code> <code>str</code> <p>Column to be used to extract input features.</p> <code>'text'</code> <code>y_col_name</code> <code>str</code> <p>Label column in the data.</p> <code>'label'</code> <p>Returns:</p> <pre><code>pd.DataFrame: Input Data samples with\n\n    1) 'predicted_label' - Predicted labels using CrossValCurate\n\n    2) 'prediction_probability' - Corresponding prediction probabilities\n\n    3) 'label_correctness_score' - Label correctness scores based on prediction probabilities\n\n    4) 'is_label_correct' - An indicator variable (True / False) classifying samples as correctly / incorrectly labelled.\n</code></pre> Source code in <code>dqc/crossval.py</code> <pre><code>@_exception_handler\ndef fit_transform(\n    self,\n    data_with_noisy_labels: pd.DataFrame,\n    X_col_name: str = \"text\",\n    y_col_name: str = \"label\",\n    **options,\n) -&gt; pd.DataFrame:\n    \"\"\"Fit CrossValCurate on the given input data\n\n    Args:\n        data_with_noisy_labels (pd.DataFrame): Input data with corresponding noisy labels\n        X_col_name (str): Column to be used to extract input features.\n        y_col_name (str): Label column in the data.\n    Returns:\n\n        pd.DataFrame: Input Data samples with \\n\n            1) 'predicted_label' - Predicted labels using CrossValCurate \\n\n            2) 'prediction_probability' - Corresponding prediction probabilities \\n\n            3) 'label_correctness_score' - Label correctness scores based on prediction probabilities \\n\n            4) 'is_label_correct' - An indicator variable (True / False) classifying samples as correctly / incorrectly labelled.\n    \"\"\"\n    _is_valid(data_with_noisy_labels, X_col_name, y_col_name)\n    n_splits = self.n_splits\n    options[\"num_samples\"] = len(data_with_noisy_labels)\n\n    # Make a copy of `data_with_noisy_labels` to prevent accidental modification\n    input_data = data_with_noisy_labels.copy()\n    logger.info(\"Pre-processing the data..\")\n    dp = _DataProcessor(\n        random_state=self.random_state,\n    )\n\n    input_data, row_id_col, y_col_name_int, inv_label_mapping = dp._preprocess(\n        input_data, y_col_name=y_col_name\n    )\n\n    # y_col_name_int needs to be accessed downstream\n    self.y_col_name_int = y_col_name_int\n\n    data_columns = [X_col_name, y_col_name_int]\n    logger.info(\n        f\"Building the curation pipeline with {n_splits}-fold cross validation..\"\n    )\n    self.curate_pipeline = _get_pipeline(\n        self.curate_feature_extractor, self.curate_model, **options\n    )\n\n    split_indices = _data_splitter(\n        input_data,\n        X_col_name=X_col_name,\n        y_col_name_int=y_col_name_int,\n        n_splits=self.n_splits,\n    )\n\n    # Lists to store predictions\n    predictions = []\n    prediction_probabilities = []\n    label_correctness_scores = []\n\n    # Keep track of shuffled data\n    row_ids = []\n\n    if self.calibration_method == \"calibrate_using_baseline\":\n        logger.info(\"Computing baseline predictions for each label..\")\n        baseline_probs = self._get_baselines(input_data[data_columns])\n\n    # Iterate through kfold splits\n    for train_index, val_index in tqdm(split_indices):\n        # Split the data\n        X_train, X_val = (\n            input_data.loc[train_index, X_col_name].values,\n            input_data.loc[val_index, X_col_name].values,\n        )\n        y_train, y_val = (\n            input_data.loc[train_index, y_col_name_int].values,\n            input_data.loc[val_index, y_col_name_int].values,\n        )\n\n        # Train the model\n        self.curate_pipeline.fit(X_train, y_train)\n        classes_ = self.curate_pipeline.classes_.tolist()\n\n        # Make predictions on the validation set\n        y_pred_probs = self.curate_pipeline.predict_proba(X_val)\n\n        if self.calibration_method == \"calibrate_using_baseline\":\n            y_preds, y_pred_probs, label_cscores = self._calibrate_using_baseline(\n                y_val,\n                y_pred_probs,\n                label_list=classes_,\n                baseline_probs=baseline_probs,\n            )\n        else:\n            y_preds, y_pred_probs, label_cscores = self._no_calibration(\n                y_val, y_pred_probs, label_list=classes_\n            )\n\n        predictions.extend(y_preds)\n        prediction_probabilities.extend(y_pred_probs)\n        label_correctness_scores.extend(label_cscores)\n        row_ids.extend(input_data.loc[val_index, row_id_col].values)\n\n    # Order dataframe according to `rowids``\n\n    row_id_df = pd.DataFrame()\n    row_id_df[row_id_col] = pd.Series(row_ids)\n    input_data = pd.merge(row_id_df, input_data, how=\"left\", on=row_id_col)\n\n    # Add results as columns\n    input_data[\"label_correctness_score\"] = pd.Series(label_correctness_scores)\n    input_data[\"predicted_label_int\"] = pd.Series(predictions)\n\n    input_data[\"prediction_probability\"] = pd.Series(prediction_probabilities)\n\n    logger.info(\"Identifying the correctly labelled samples..\")\n    input_data[\"is_label_correct\"] = input_data.progress_apply(\n        self._is_confident, axis=1\n    )\n\n    return dp._postprocess(\n        input_data,\n        display_cols=list(data_with_noisy_labels.columns) + self.result_col_list,\n        inv_label_mapping=inv_label_mapping,\n    )\n</code></pre>"},{"location":"api/util/utils/","title":"Util","text":""},{"location":"api/util/utils/#add_asymmetric_noise","title":"add_asymmetric_noise","text":"<p>Util function to add asymmetric noise to labels for simulation of noisy label scenarios.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Series</code> <p>Input pandas series with integer values             ranging from 0 to n - 1.</p> required <code>noise_prob</code> <code>float</code> <p>Probability of adding noise to each value.</p> required <code>random_state</code> <code>Union[RandomState, None]</code> <p>Random seed for reproducibility</p> <code>42</code> <p>Returns:     pd.Series: Series with asymmetric noise added to it.     float: Normalized quantification of pairwise disagreement between <code>labels</code> and <code>noisy_labels</code> for parity check</p> Source code in <code>dqc/utils/noise_utils.py</code> <pre><code>def add_asymmetric_noise(\n    labels: pd.Series,\n    noise_prob: float,\n    random_state: Union[RandomState, None] = 42,\n) -&gt; Tuple[pd.Series, float]:\n    \"\"\"\n    Util function to add asymmetric noise to labels\n    for simulation of noisy label scenarios.\n\n    Args:\n        labels (pd.Series): Input pandas series with integer values\n                        ranging from 0 to n - 1.\n        noise_prob (float): Probability of adding noise to each value.\n        random_state (Union[RandomState, None]): Random seed for reproducibility\n    Returns:\n        pd.Series: Series with asymmetric noise added to it.\n        float: Normalized quantification of pairwise disagreement between `labels` and `noisy_labels` for parity check\n    \"\"\"\n    # Set seed\n    np.random.seed(random_state)\n\n    # Avoid modifying the original data\n    noisy_labels = labels.copy()\n\n    # Build a replacement dictionary\n    unique_labels = list(set(noisy_labels))\n    replacement_dict = {\n        label: [candidate for candidate in unique_labels if candidate != label]\n        for label in unique_labels\n    }\n\n    # Determine the number of samples to modify based on the noise probability\n    num_samples = min(len(noisy_labels), int(len(noisy_labels) * noise_prob + 1))\n\n    # Sample random indices from the labels to introduce noise\n    target_indices = np.random.choice(len(noisy_labels), num_samples, replace=False)\n\n    for idx in target_indices:\n        # Introduce noise\n        noisy_labels[idx] = np.random.choice(replacement_dict[noisy_labels[idx]])\n\n    # Parity check\n    num_mismatches = sum(\n        [\n            label != noisy_label\n            for label, noisy_label in zip(labels.values, noisy_labels.values)\n        ]\n    )\n    observed_noise_ratio = num_mismatches / len(noisy_labels)\n\n    return noisy_labels, observed_noise_ratio\n</code></pre>"},{"location":"api/util/utils/#show_versions","title":"show_versions","text":"<p>Print useful debugging information</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary object containing system information</p> Source code in <code>dqc/version.py</code> <pre><code>def show_versions():\n    \"\"\"Print useful debugging information\n\n    Returns:\n        dict: Dictionary object containing system information\n    \"\"\"\n    versions = {\n        \"os_type\": platform.system(),\n        \"os_version\": platform.release(),\n        \"python_version\": platform.python_version(),\n        \"dqc-toolkit\": __version__,\n        \"transformers\": transformers.__version__,\n        \"sentence_transformers\": sentence_transformers.__version__,\n        \"scikit-learn\": sklearn.__version__,\n    }\n\n    return versions\n</code></pre>"}]}